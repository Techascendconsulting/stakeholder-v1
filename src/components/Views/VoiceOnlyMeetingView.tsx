import React, { useState, useEffect, useRef } from 'react';
import { ArrowLeft, Mic, MicOff, Send, Users, Clock, Volume2, Play, Pause, Square, Phone, PhoneOff, Settings, MoreVertical, ChevronDown, ChevronUp, X } from 'lucide-react';
import { useApp } from '../../contexts/AppContext';
import { useAuth } from '../../contexts/AuthContext';
import { useVoice } from '../../contexts/VoiceContext';
import { Message } from '../../types';
import AIService, { StakeholderContext, ConversationContext } from '../../services/aiService';
import { murfTTS } from '../../services/murfTTS';
import { playBrowserTTS } from '../../lib/browserTTS';
import { transcribeWithDeepgram, getSupportedDeepgramFormats } from '../../lib/deepgram';
import { createDeepgramStreaming, DeepgramStreaming } from '../../lib/deepgramStreaming';
// import StreamingTTSService from '../../services/streamingTTS'; // Disabled for Murf TTS
import { DatabaseService } from '../../lib/database';
import { UserAvatar } from '../Common/UserAvatar';
import { getUserProfilePhoto, getUserDisplayName } from '../../utils/profileUtils';

interface ParticipantCardProps {
  participant: any;
  isCurrentSpeaker: boolean;
  isUser?: boolean;
}

const ParticipantCard: React.FC<ParticipantCardProps> = ({ 
  participant, 
  isCurrentSpeaker, 
  isUser = false 
}) => {
  const { user } = useAuth();
  
  const getInitials = (name: string) => {
    return name.split(' ').map(n => n[0]).join('').toUpperCase().slice(0, 2);
  };

  // Get stakeholder photo from selectedStakeholders or participant data
  const getParticipantPhoto = () => {
    if (isUser) return null;
    
    // Try participant.photo first
    if (participant.photo) return participant.photo;
    
    // Try to find in selectedStakeholders
    const stakeholder = selectedStakeholders?.find(s => 
      s.name === participant.name || s.id === participant.id
    );
    return stakeholder?.photo || null;
  };

  const photo = getParticipantPhoto();

  return (
    <div className="relative bg-gray-800 rounded-xl overflow-hidden group hover:bg-gray-750 transition-colors border border-gray-700 w-64 h-48">
      {/* Animated Speaking Ring */}
      {isCurrentSpeaker && (
        <div className="absolute inset-0 rounded-xl border-4 border-green-400 animate-pulse z-10">
          <div className="absolute inset-0 rounded-xl border-4 border-green-400 opacity-50 animate-ping"></div>
        </div>
      )}
      
      {/* Video/Photo Content */}
      {isUser ? (
        <div className="w-full h-full flex items-center justify-center relative">
          {getUserProfilePhoto(user?.id || '') ? (
            /* User's uploaded profile photo displayed as full photo like other participants */
            <img
              src={getUserProfilePhoto(user?.id || '') || ''}
              alt={getUserDisplayName(user?.id || '', user?.email)}
              className="w-full h-full object-cover"
            />
          ) : (
            /* Fallback to gradient background with initials if no profile photo */
            <div className="flex flex-col items-center justify-center h-full w-full bg-gradient-to-br from-indigo-500 to-purple-600">
              <div className="w-16 h-16 rounded-full bg-white/20 flex items-center justify-center text-white text-xl font-bold mb-2">
                {getUserDisplayName(user?.id || '', user?.email)?.charAt(0)?.toUpperCase() || 'U'}
              </div>
              <div className="text-white text-xs font-medium opacity-75">
                Business Analyst
              </div>
            </div>
          )}
        </div>
      ) : photo ? (
        <img
          src={photo}
          alt={participant.name}
          className="w-full h-full object-cover"
        />
      ) : (
        <div className="w-full h-full flex items-center justify-center bg-red-500">
          <span className="text-white text-6xl font-bold">
            {getInitials(participant.name)}
          </span>
        </div>
      )}
      
      {/* Name overlay */}
      <div className="absolute bottom-2 left-2 bg-black bg-opacity-60 text-white px-2 py-1 rounded text-sm">
        {participant.name}
      </div>

             {/* Speaking/Mute indicator */}
       {!isUser && (
         <div className="absolute top-2 left-2">
           {isCurrentSpeaker ? (
             <div className="bg-green-500 p-1 rounded-full animate-pulse">
               <Volume2 className="w-3 h-3 text-white" />
             </div>
           ) : (
             <div className="bg-gray-600 p-1 rounded-full">
               <MicOff className="w-3 h-3 text-white" />
             </div>
           )}
         </div>
       )}
    </div>
  );
};

interface SpeakingQueueHeaderProps {
  currentSpeaker: string | null;
  upcomingQueue: { name: string; id?: string }[];
  selectedStakeholders: any[];
}

const SpeakingQueueHeader: React.FC<SpeakingQueueHeaderProps> = ({ 
  currentSpeaker, 
  upcomingQueue, 
  selectedStakeholders 
}) => {
  if (!currentSpeaker && upcomingQueue.length === 0) return null;
  
  const getCurrentSpeakerInfo = () => {
    if (!currentSpeaker) return null;
    return selectedStakeholders.find(s => s.name === currentSpeaker) || { name: currentSpeaker, role: 'Participant' };
  };

  const getUpcomingSpeakerInfo = (speakerName: string) => {
    return selectedStakeholders.find(s => s.name === speakerName) || { name: speakerName, role: 'Participant' };
  };

  const currentSpeakerInfo = getCurrentSpeakerInfo();
  
  return (
    <div className="bg-gradient-to-r from-gray-800/90 to-gray-700/90 backdrop-blur-sm rounded-xl px-6 py-4 mx-6 mb-4 border border-gray-600/50 shadow-lg">
      <div className="flex items-center justify-between">
        {/* Current Speaker */}
        {currentSpeakerInfo && (
          <div className="flex items-center space-x-3">
            <div className="flex items-center space-x-2">
              <div className="relative">
                <div className="w-3 h-3 bg-green-400 rounded-full animate-pulse"></div>
                <div className="absolute inset-0 w-3 h-3 bg-green-400 rounded-full animate-ping opacity-50"></div>
              </div>
              <div className="flex flex-col">
                <span className="text-green-400 text-sm font-semibold">Currently Speaking</span>
                <div className="flex items-center space-x-2">
                  <span className="text-white font-medium">{currentSpeakerInfo.name}</span>
                  <span className="text-gray-300 text-xs">‚Ä¢</span>
                  <span className="text-gray-300 text-xs">{currentSpeakerInfo.role}</span>
                </div>
              </div>
            </div>
          </div>
        )}
        
        {/* Speaking Queue */}
        {upcomingQueue.length > 0 && (
          <div className="flex items-center space-x-4">
            <div className="flex items-center space-x-2">
              <div className="w-2 h-2 bg-blue-400 rounded-full"></div>
              <span className="text-blue-400 text-sm font-medium">Speaking Next:</span>
            </div>
            <div className="flex items-center space-x-3">
              {upcomingQueue.slice(0, 3).map((speaker, index) => {
                const speakerInfo = getUpcomingSpeakerInfo(speaker.name);
                return (
                  <div key={index} className="flex items-center space-x-1">
                    <div className="bg-gray-600/80 rounded-lg px-3 py-1.5 border border-gray-500/50">
                      <div className="flex flex-col">
                        <span className="text-white text-xs font-medium">{speakerInfo.name}</span>
                        <span className="text-gray-300 text-xs">{speakerInfo.role}</span>
                      </div>
                    </div>
                    {index < Math.min(upcomingQueue.length - 1, 2) && (
                      <div className="w-1 h-1 bg-gray-500 rounded-full"></div>
                    )}
                  </div>
                );
              })}
              {upcomingQueue.length > 3 && (
                <div className="bg-gray-600/60 rounded-lg px-2 py-1.5 border border-gray-500/30">
                  <span className="text-gray-400 text-xs">+{upcomingQueue.length - 3} more</span>
                </div>
              )}
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

// Helper function to split text into natural speech chunks
const splitIntoNaturalChunks = (text: string): string[] => {
  // Split by sentence-ending punctuation first
  const sentences = text.split(/([.!?]+\s*)/).filter(s => s.trim().length > 0);
  const chunks: string[] = [];
  let currentChunk = '';
  
  for (const sentence of sentences) {
    // If adding this sentence would make chunk too long, start new chunk
    if (currentChunk && (currentChunk + sentence).length > 100) {
      chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += sentence;
    }
    
    // If sentence ends with punctuation, it's a good break point
    if (sentence.match(/[.!?]+\s*$/)) {
      chunks.push(currentChunk.trim());
      currentChunk = '';
    }
  }
  
  // Add any remaining text
  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }
  
  // If no natural breaks found, fall back to phrase-based chunking
  if (chunks.length === 0 || chunks[0].length > 150) {
    return splitByPhrases(text);
  }
  
  return chunks.filter(chunk => chunk.length > 0);
};

// Fallback: Split by phrases and natural pauses
const splitByPhrases = (text: string): string[] => {
  // Split by commas, conjunctions, and other natural pause points
  const phrases = text.split(/(\s*[,;]\s*|\s+(?:and|but|or|so|because|when|while|if|although)\s+)/i);
  const chunks: string[] = [];
  let currentChunk = '';
  
  for (const phrase of phrases) {
    if ((currentChunk + phrase).length > 80 && currentChunk.trim()) {
      chunks.push(currentChunk.trim());
      currentChunk = phrase;
    } else {
      currentChunk += phrase;
    }
  }
  
  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }
  
  return chunks.filter(chunk => chunk.length > 0);
};

export const VoiceOnlyMeetingView: React.FC = () => {
  const { selectedProject, selectedStakeholders, setCurrentView, user, setSelectedMeeting } = useApp();
  const { globalAudioEnabled, getStakeholderVoice, isStakeholderVoiceEnabled } = useVoice();
  
  // Component-level aiService instance
  const aiService = AIService.getInstance();

  // Debug component mount
  useEffect(() => {
    console.log('üöÄ VoiceOnlyMeetingView MOUNTED - Initial state:', {
      selectedProject: selectedProject?.name || 'NONE',
      selectedStakeholdersCount: selectedStakeholders?.length || 0,
      user: user?.email || 'NONE',
      userId: user?.id || 'NONE'
    });
  }, []);
  
  // State management (same as before)
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputMessage, setInputMessage] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [currentAudio, setCurrentAudio] = useState<HTMLAudioElement | null>(null);
  const [playingMessageId, setPlayingMessageId] = useState<string | null>(null);
  const [audioStates, setAudioStates] = useState<{ [key: string]: 'loading' | 'playing' | 'stopped' }>({});
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [currentSpeaker, setCurrentSpeaker] = useState<any>(null);
  
  // New streaming voice input state
  const [isListening, setIsListening] = useState(false);
  const [liveTranscript, setLiveTranscript] = useState('');
  const [finalTranscript, setFinalTranscript] = useState('');
  const [streamingService, setStreamingService] = useState<DeepgramStreaming | null>(null);
  
  // Dynamic UX state management
  const [isGeneratingResponse, setIsGeneratingResponse] = useState(false);
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);
  const [dynamicFeedback, setDynamicFeedback] = useState<string | null>(null);
  const [responseQueue, setResponseQueue] = useState<{
    current: string | null;
    upcoming: { name: string; id?: string }[];
  }>({ current: null, upcoming: [] });
  
  // Speaking queue management - EXACT REPLICA of transcript meeting
  const [conversationQueue, setConversationQueue] = useState<string[]>([]);
  const [currentSpeaking, setCurrentSpeaking] = useState<string | null>(null);
  
  // Speaking queue for parallel processing
  const [speakingQueueState, setSpeakingQueueState] = useState<Array<{
    stakeholder: any;
    response: string;
    responseMessage: any;
    audioBlob: Blob | null;
    index: number;
  }>>([]);

  // Transcription toggle and panel state
  const [transcriptionEnabled, setTranscriptionEnabled] = useState(false);
  const [transcriptPanelOpen, setTranscriptPanelOpen] = useState(false);
  const [transcriptMessages, setTranscriptMessages] = useState<Message[]>([]);

  // Background transcript capture (always active)
  const [backgroundTranscript, setBackgroundTranscript] = useState<Message[]>([]);
  const [meetingId, setMeetingId] = useState<string | null>(null);
  const [meetingStartTime, setMeetingStartTime] = useState(Date.now());
  
  // Meeting ending states to prevent multiple clicks and show progress
  const [isEndingMeeting, setIsEndingMeeting] = useState(false);
  const [endingProgress, setEndingProgress] = useState('');
  
  // Reset meeting state when component mounts to ensure fresh meeting
  useEffect(() => {
    console.log('üîÑ VoiceOnlyMeetingView - Component mounted, resetting meeting state');
    setMeetingId(null);
    setMeetingStartTime(Date.now());
    setBackgroundTranscript([]);
    setMessages([]);
    setTranscriptMessages([]);
    setIsEndingMeeting(false);
    setEndingProgress('');
  }, []); // Only run on mount

  // Ensure meeting ID exists - safety mechanism
  useEffect(() => {
    if (!meetingId && user?.id) {
      const safeMeetingId = `safe-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      setMeetingId(safeMeetingId);
      setMeetingStartTime(Date.now());
      console.log('üõ°Ô∏è SAFETY: Created meeting ID because none existed:', safeMeetingId);
    }
  }, [meetingId, user?.id]);

  // Add conversation dynamics from transcript meeting for adaptive responses
  const [conversationDynamics, setConversationDynamics] = useState({
    phase: 'initial' as 'initial' | 'introduction_active' | 'discussion_active',
    greetingIterations: 0,
    leadSpeaker: null as any,
    introducedMembers: new Set<string>()
  });

  // Background transcript capture function (always captures, regardless of UI)
  const addToBackgroundTranscript = (message: Message) => {
    console.log('üìù BACKGROUND TRANSCRIPT - Adding message:', {
      speaker: message.speaker,
      stakeholderName: message.stakeholderName,
      contentLength: message.content.length,
      timestamp: message.timestamp
    });
    
    setBackgroundTranscript(prev => {
      const newTranscript = [...prev, message];
      console.log('üìù BACKGROUND TRANSCRIPT - New length:', newTranscript.length);
      return newTranscript;
    });
    
    // Also add to visible transcript if enabled
    if (transcriptionEnabled) {
      setTranscriptMessages(prev => [...prev, message]);
    }
  };

  // Initialize meeting in database
  useEffect(() => {
    const initializeMeeting = async () => {
      console.log('üéØ INIT MEETING - Checking conditions:', {
        hasSelectedProject: !!selectedProject,
        stakeholdersCount: selectedStakeholders?.length || 0,
        hasUserId: !!user?.id,
        hasExistingMeetingId: !!meetingId
      });
      
      // More lenient conditions - even if stakeholders is empty, try to create meeting
      if (selectedProject && user?.id && !meetingId) {
        try {
          // Use fallback values if stakeholders are missing
          const stakeholderIds = selectedStakeholders?.map(s => s.id) || ['default-stakeholder'];
          const stakeholderNames = selectedStakeholders?.map(s => s.name) || ['Default Stakeholder'];
          const stakeholderRoles = selectedStakeholders?.map(s => s.role) || ['Business Stakeholder'];
          
          console.log('üéØ INIT MEETING - Creating meeting with:', {
            userId: user.id,
            projectId: selectedProject.id,
            projectName: selectedProject.name,
            stakeholderIds,
            stakeholderNames,
            stakeholderRoles
          });
          
          const newMeetingId = await DatabaseService.createMeeting(
            user.id,
            selectedProject.id,
            selectedProject.name,
            stakeholderIds,
            stakeholderNames,
            stakeholderRoles,
            'voice-only'
          );
          
          console.log('üéØ INIT MEETING - DatabaseService.createMeeting result:', newMeetingId);
          
          if (newMeetingId) {
            setMeetingId(newMeetingId);
            setMeetingStartTime(Date.now());
            console.log('üéØ Meeting initialized in database:', newMeetingId);
          } else {
            console.error('üéØ INIT MEETING - Failed to create meeting, no ID returned');
            // Create fallback local meeting ID
            const fallbackMeetingId = `local-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
            setMeetingId(fallbackMeetingId);
            setMeetingStartTime(Date.now());
            console.log('üéØ Created fallback meeting ID:', fallbackMeetingId);
          }
        } catch (error) {
          console.error('üéØ INIT MEETING - Error initializing meeting:', error);
          // Create emergency fallback meeting ID
          const emergencyMeetingId = `emergency-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
          setMeetingId(emergencyMeetingId);
          setMeetingStartTime(Date.now());
          console.log('üéØ Created emergency meeting ID:', emergencyMeetingId);
        }
      } else {
        console.log('üéØ INIT MEETING - Conditions not met:', {
          hasSelectedProject: !!selectedProject,
          hasUserId: !!user?.id,
          alreadyHasMeetingId: !!meetingId
        });
        
        // If we don't have a meeting ID yet, create one anyway to ensure saving works
        if (!meetingId && user?.id) {
          const backupMeetingId = `backup-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
          setMeetingId(backupMeetingId);
          setMeetingStartTime(Date.now());
          console.log('üéØ Created backup meeting ID for conditions not met:', backupMeetingId);
        }
      }
    };

    // Small delay to ensure all context is loaded
    const timeoutId = setTimeout(initializeMeeting, 500);
    return () => clearTimeout(timeoutId);
  }, [selectedProject, selectedStakeholders, user?.id, meetingId]);

  // Generate stakeholder response with context - EXACT COPY from transcript meeting
  const generateStakeholderResponse = async (stakeholder: any, userMessage: string, currentMessages: Message[], responseType: string) => {
    const stakeholderContext = {
      name: stakeholder.name,
      role: stakeholder.role,
      department: stakeholder.department,
      priorities: stakeholder.priorities,
      personality: stakeholder.personality,
      expertise: stakeholder.expertise || []
    };

    const conversationContext = {
      project: {
        name: selectedProject?.name || 'Current Project',
        description: selectedProject?.description || 'Project description',
        type: selectedProject?.type || 'General'
      },
      conversationHistory: currentMessages,
      stakeholders: selectedStakeholders.map(s => ({
        name: s.name,
        role: s.role,
        department: s.department,
        priorities: s.priorities,
        personality: s.personality,
        expertise: s.expertise || []
      }))
    };

    const aiService = AIService.getInstance();
    return await aiService.generateStakeholderResponse(
      userMessage,
      stakeholderContext,
      conversationContext,
      responseType
    );
  };

  // Create response message object - EXACT COPY from transcript meeting
  const createResponseMessage = (stakeholder: any, response: string, index: number): Message => {
    return {
      id: `ai-${Date.now()}-${index}`,
      speaker: stakeholder.id || 'stakeholder',
      content: response,
      timestamp: new Date().toISOString(),
      stakeholderName: stakeholder.name,
      stakeholderRole: stakeholder.role
    };
  };

  // Dynamic contextually-aligned thinking message generator - EXACT COPY from transcript meeting
  const generateThinkingMessage = (stakeholder: any, context: any) => {
    const userQuestion = context.messageContent || '';
    const responseContext = context.responseContext || '';
    
    // Simple thinking message based on context
    if (responseContext === 'baton_pass') {
      return `${stakeholder.name} is considering the question...`;
    } else if (responseContext === 'introduction_lead') {
      return `${stakeholder.name} is preparing to introduce the team...`;
    } else {
      return `${stakeholder.name} is thinking about your question...`;
    }
  };

  // STREAMING: Real-time stakeholder response with immediate audio playback
  const processSingleStakeholderStreaming = async (stakeholder: any, messageContent: string, currentMessages: Message[], responseContext: string) => {
    console.log(`üé§ STREAMING: Starting real-time response for ${stakeholder.name}`);
    
    try {
      // Disabled streaming TTS for Murf - use complete response instead
      let fullResponse = '';
      
      // Set current speaker immediately
      setCurrentSpeaker(stakeholder);
      setCurrentSpeaking(stakeholder.id);
      
      // Generate GPT response with streaming
      const aiService = AIService.getInstance();
      const stakeholderContext = {
        name: stakeholder.name,
        role: stakeholder.role,
        department: stakeholder.department,
        priorities: stakeholder.priorities,
        personality: stakeholder.personality,
        expertise: stakeholder.expertise || []
      };

      const conversationContext = {
        project: {
          name: selectedProject?.name || 'Current Project',
          description: selectedProject?.description || 'Project description',
          type: selectedProject?.type || 'General'
        },
        conversationHistory: currentMessages,
        stakeholders: selectedStakeholders.map(s => ({
          name: s.name,
          role: s.role,
          department: s.department,
          priorities: s.priorities,
          personality: s.personality,
          expertise: s.expertise || []
        }))
      };
      
      const response = await aiService.generateStakeholderResponseStreaming(
        messageContent,
        stakeholderContext,
        conversationContext,
        responseContext as any,
        (chunk: string) => {
          // This callback fires for each text chunk from GPT
          console.log(`üìù Received chunk: "${chunk}"`);
          fullResponse += chunk + ' ';
          // Note: Streaming TTS disabled - will use complete response
        }
      );
      
      // Create message object
      const responseMessage: Message = {
        id: `ai-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        speaker: stakeholder.id,
        content: response,
        timestamp: new Date().toISOString(),
        stakeholderName: stakeholder.name
      };

      // Add message immediately
      const updatedMessages = [...currentMessages, responseMessage];
      setMessages(updatedMessages);
      addToBackgroundTranscript(responseMessage);

      // Generate and play audio immediately
      if (globalAudioEnabled) {
        const voiceName = stakeholder.voice;
        if (murfTTS.isConfigured()) {
          console.log(`üéµ FAST: Generating voice for ${stakeholder.name} with Murf TTS`);
          const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
          
          if (audioBlob) {
            setCurrentSpeaker(stakeholder);
            setCurrentSpeaking(stakeholder.id);
            
            const audioUrl = URL.createObjectURL(audioBlob);
            const audio = new Audio(audioUrl);
            
            setCurrentAudio(audio);
            setPlayingMessageId(responseMessage.id);
            setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'playing' }));
            
            await new Promise((resolve) => {
              audio.onended = () => {
                URL.revokeObjectURL(audioUrl);
                setCurrentAudio(null);
                setPlayingMessageId(null);
                setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'stopped' }));
                setCurrentSpeaker(null);
                setCurrentSpeaking(null);
                console.log(`üöÄ FAST: ${stakeholder.name} completed`);
                resolve(void 0);
              };
              
              audio.onerror = () => {
                URL.revokeObjectURL(audioUrl);
                setCurrentAudio(null);
                setPlayingMessageId(null);
                setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'stopped' }));
                setCurrentSpeaker(null);
                setCurrentSpeaking(null);
                resolve(void 0);
              };
              
              audio.play().catch(() => resolve(void 0));
            });
          } else {
            console.warn('‚ùå Murf TTS returned null, falling back to browser TTS');
            await playBrowserTTS(response);
          }
        } else {
          console.log('‚ö†Ô∏è Murf TTS not configured, using browser TTS');
          await playBrowserTTS(response);
        }
      }
      
      console.log(`‚úÖ FAST: ${stakeholder.name} response completed`);
      
    } catch (error) {
      console.error(`‚ùå FAST: Error processing ${stakeholder.name}:`, error);
    }
    
    // Clear any persistent feedback messages
    setDynamicFeedback(null);
  };

  // NEW: Streaming parallel processing - stakeholders start speaking as soon as ready
  const processStakeholdersInParallel = async (mentionedStakeholders: any[], messageContent: string, currentMessages: Message[], responseContext: string) => {
    // Prevent multiple simultaneous processing
    if (isProcessingRef.current) {
      console.log('üö´ PARALLEL: Already processing stakeholders, skipping duplicate call');
      return;
    }
    
    isProcessingRef.current = true;
    console.log(`üöÄ STREAMING: Processing ${mentionedStakeholders.length} stakeholders with streaming responses`);
    console.log(`üîç STAKEHOLDER DEBUG: mentionedStakeholders:`, mentionedStakeholders.map(s => s.name));
    
    try {
      // Use the state-managed speaking queue for proper stop button control
      setSpeakingQueueState([]); // Clear any existing queue
    
    let completedCount = 0;
    let workingMessages = currentMessages;
    let isSpeaking = false; // Synchronous lock to prevent simultaneous speaking
    let localQueue: any[] = []; // Synchronous local queue to avoid React state timing issues
    
          // Function to process the next item in the speaking queue
      const processNextInQueue = async () => {
        if (isSpeaking || localQueue.length === 0) {
          console.log(`üîç QUEUE DEBUG: Skipping processNextInQueue - isSpeaking: ${isSpeaking}, localQueueLength: ${localQueue.length}`);
          return;
        }
        
        isSpeaking = true; // Set synchronous lock immediately
        setCurrentSpeaking('speaking');
        
        // Get and remove the first item from the LOCAL queue
        const nextItem = localQueue.shift(); // Remove first item synchronously
        console.log(`üîç QUEUE DEBUG: Processing ${nextItem.stakeholder.name}, remaining local queue length: ${localQueue.length}`);
      const { stakeholder, response, responseMessage, audioBlob, index } = nextItem;
      
      console.log(`üéµ STREAMING: ${stakeholder.name} starting to speak (${index + 1}/${mentionedStakeholders.length})`);
      
      // Add message to state immediately
      workingMessages = [...workingMessages, responseMessage];
      setMessages(workingMessages);
      addToBackgroundTranscript(responseMessage);
      
      // Set current speaker for UI
      setCurrentSpeaker(stakeholder);
      setCurrentSpeaking(stakeholder.id);
      
      // Generate and play audio using Murf TTS (simple and reliable)
      if (globalAudioEnabled && response) {
        try {
          console.log(`üé§ MURF: Generating audio for ${stakeholder.name}`);
          
          setPlayingMessageId(responseMessage.id);
          setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'playing' }));
          
          // Generate audio with Murf TTS
          const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
          
          if (audioBlob) {
            console.log(`üéµ MURF: Playing audio for ${stakeholder.name}`);
            
            // Play the audio
            await murfTTS.playAudio(audioBlob);
            
            console.log(`‚úÖ MURF: ${stakeholder.name} finished speaking`);
          } else {
            console.error(`‚ùå MURF: Failed to generate audio for ${stakeholder.name}`);
          }
          
          setCurrentAudio(null);
          setPlayingMessageId(null);
          setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'stopped' }));
          
        } catch (error) {
          console.error(`‚ùå MURF: Error for ${stakeholder.name}:`, error);
          setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'stopped' }));
        }
      }
      // Fallback for non-real-time responses
      else if (globalAudioEnabled && response) {
        try {
          const voiceName = stakeholder.voice;
          if (murfTTS.isConfigured()) {
            console.log(`üé§ Using Murf TTS for ${stakeholder.name}`);
            
            const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
            if (audioBlob) {
              const audioUrl = URL.createObjectURL(audioBlob);
              const audio = new Audio(audioUrl);
              
              setCurrentAudio(audio);
              setPlayingMessageId(responseMessage.id);
              setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'playing' }));
              
              await new Promise<void>((resolve) => {
                audio.onended = () => {
                  URL.revokeObjectURL(audioUrl);
                  setCurrentAudio(null);
                  setPlayingMessageId(null);
                  setAudioStates(prev => ({ ...prev, [responseMessage.id]: 'stopped' }));
                  resolve();
                };
                audio.play().catch(() => resolve());
              });
            } else {
              console.warn('‚ùå Murf TTS returned null, falling back to browser TTS');
              await playBrowserTTS(response);
            }
          } else {
            console.log('‚ö†Ô∏è Murf TTS not configured, using browser TTS');
            await playBrowserTTS(response);
          }
        } catch (error) {
          console.error(`‚ùå Fallback TTS error for ${stakeholder.name}:`, error);
        }
      }
      
      // Clear current speaker
      setCurrentSpeaker(null);
      setCurrentSpeaking(null);
      
      isSpeaking = false; // Release synchronous lock
      
      console.log(`üîç QUEUE DEBUG: ${stakeholder.name} finished speaking. LOOP PREVENTION: Not calling processNextInQueue recursively.`);
      
      // DISABLED: This recursive call was causing infinite loops
      // setTimeout(() => {
      //   if (!isSpeaking && speakingQueueState.length > 0) {
      //     console.log(`üîç QUEUE DEBUG: Triggering next item after ${stakeholder.name} finished`);
      //     processNextInQueue();
      //   }
      // }, 50);
    };
    
    // Start parallel generation for all stakeholders
    const stakeholderPromises = mentionedStakeholders.map(async (mentionedStakeholderContext, index) => {
      console.log(`üîç STAKEHOLDER DEBUG: Processing index ${index}: ${mentionedStakeholderContext.name}`);
      const stakeholder = selectedStakeholders.find(s => s.name === mentionedStakeholderContext.name);
      
      if (!stakeholder) {
        console.log(`‚ùå Could not find stakeholder object for: ${mentionedStakeholderContext.name}`);
        return null;
      }

      console.log(`‚ö° PARALLEL: Starting GPT + Voice generation for ${stakeholder.name}`);
      
      try {
        // Promise-level guard to prevent duplicate execution
        const promiseId = `${stakeholder.name}-${index}-${Date.now()}`;
        console.log(`üîç PROMISE DEBUG: Starting promise ${promiseId} for ${stakeholder.name}`);
        
        // Create message object for transcript
        const responseMessage = createResponseMessage(stakeholder, '', currentMessages.length + index);
        
        console.log(`üöÄ MURF: Starting AI generation for ${stakeholder.name}`);
        
        // Generate AI response first, then add to speaking queue
        const stakeholderContext = {
          name: stakeholder.name,
          role: stakeholder.role,
          department: stakeholder.department,
          priorities: stakeholder.priorities,
          personality: stakeholder.personality,
          expertise: stakeholder.expertise || []
        };
        
        const response = await aiService.generateStakeholderResponse(
          messageContent,
          stakeholderContext,
          { 
            project: selectedProject!,
            conversationHistory: workingMessages,
            conversationPhase: 'as_is'
          },
          'direct_mention'
        );
        
        // Update response message with generated content
        responseMessage.content = response;
        
        const queueItem = {
          stakeholder,
          response,
          responseMessage,
          audioBlob: null,
          index
        };
        
        setSpeakingQueueState(prev => {
          // Check if this stakeholder is already in the queue
          const alreadyInQueue = prev.some(item => item.stakeholder.name === stakeholder.name);
          if (alreadyInQueue) {
            console.log(`üö´ QUEUE DEBUG: ${stakeholder.name} already in queue, skipping duplicate`);
            return prev;
          }
          
          console.log(`üîç QUEUE DEBUG: Adding ${stakeholder.name} (${promiseId}) to queue. Current queue length: ${prev.length}, new length will be: ${prev.length + 1}`);
          const newQueue = [...prev, queueItem];
          
          // Process queue after state update with minimal delay
          setTimeout(() => {
            console.log(`üîç QUEUE DEBUG: Processing queue after state update for ${stakeholder.name}`);
            processNextInQueue();
          }, 10);
          
          return newQueue;
        });
        completedCount++;
        
        console.log(`üìù STREAMING: Added ${stakeholder.name} to speaking queue (${completedCount}/${mentionedStakeholders.length} ready)`);
        
        return queueItem;
        
      } catch (error) {
        console.error(`‚ùå STREAMING: Error processing ${stakeholder.name}:`, error);
        completedCount++;
        return null;
      }
    });

    // Wait for all stakeholders to complete generation (but speaking happens in parallel)
    console.log(`‚è≥ STREAMING: All stakeholders generating responses in parallel...`);
    await Promise.all(stakeholderPromises);
    
    console.log(`‚úÖ STREAMING: All stakeholders completed generation! Speaking queue will handle playback order.`);
    
    // Note: Speaking happens automatically via the streaming queue as each stakeholder completes
    // No need to wait - the first ready stakeholder starts speaking immediately
    // while others continue generating in the background
    
    } finally {
      isProcessingRef.current = false;
      console.log(`üîì PARALLEL: Processing lock released`);
    }
  };

  // Enhanced stakeholder response processing - EXACT COPY from transcript meeting
  const processDynamicStakeholderResponse = async (stakeholder: any, messageContent: string, currentMessages: Message[], responseContext: string): Promise<Message[]> => {
    console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} starting processDynamicStakeholderResponse`);
    console.log(`üöÄ QUEUE DEBUG: Current speaker before: ${currentSpeaking}`);
    console.log(`üöÄ QUEUE DEBUG: Current queue before: [${conversationQueue.join(', ')}]`);
    
    try {
      // Add to conversation queue to prevent simultaneous speaking
      setConversationQueue(prev => {
        const newQueue = [...prev, stakeholder.id];
        console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} added to queue. New queue: [${newQueue.join(', ')}]`);
        return newQueue;
      });
      
      // Wait for turn if someone else is speaking
      let waitCount = 0;
      while (currentSpeaking !== null && currentSpeaking !== stakeholder.id) {
        waitCount++;
        console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} waiting (attempt ${waitCount}). Current speaker: ${currentSpeaking}`);
        await new Promise(resolve => setTimeout(resolve, 100));
        
        // Safety break after 100 attempts (10 seconds)
        if (waitCount > 100) {
          console.error(`üö® QUEUE ERROR: ${stakeholder.name} waited too long! Breaking wait loop.`);
          break;
        }
      }
      
      // Start speaking
      console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} now taking turn to speak`);
      setCurrentSpeaking(stakeholder.id);
      
      // Generate dynamic thinking message based on actual user question context
      const thinkingContext = {
        stakeholder,
        messageContent,
        conversationHistory: currentMessages,
        responseContext
      };
      const thinkingMessage = generateThinkingMessage(stakeholder, thinkingContext);
      setDynamicFeedback(thinkingMessage);
      
      // Generate contextual response using AI service
      const response = await generateStakeholderResponse(stakeholder, messageContent, currentMessages, responseContext);
      
      // Clean up thinking state - ensure proper cleanup
      setDynamicFeedback(null);
      
      // Create and add message with dynamic indexing
      const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length);
      let updatedMessages = [...currentMessages, responseMessage];
      setMessages(updatedMessages);
      
      // Add to background transcript (always captured)
      addToBackgroundTranscript(responseMessage);
      
      // Force cleanup of thinking state to prevent display issues
      setTimeout(() => {
        setDynamicFeedback(null);
      }, 100);
      
      // Dynamic audio handling based on user preferences and context - EXACT COPY from transcript meeting
      if (globalAudioEnabled) {
        await playMessageAudio(responseMessage.id, response, stakeholder, true).catch(console.warn);
      }
      
      // Check for traditional baton passing (keep existing functionality)
      const batonPassedStakeholder = detectBatonPassing(response, updatedMessages);
      
      // Finish speaking
      console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} finished speaking, clearing currentSpeaking`);
      setCurrentSpeaking(null);
      setConversationQueue(prev => {
        const newQueue = prev.filter(id => id !== stakeholder.id);
        console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} removed from queue. New queue: [${newQueue.join(', ')}]`);
        return newQueue;
      });
      
      // Handle traditional baton passing if detected and no mentions were processed
      if (batonPassedStakeholder && !updatedMessages.find(msg => 
        msg.id.startsWith('mention-response') && 
        msg.timestamp > responseMessage.timestamp
      )) {
        console.log(`üìã Traditional baton passed to ${batonPassedStakeholder.name}`);
        setTimeout(async () => {
          await processDynamicStakeholderResponse(batonPassedStakeholder, messageContent, updatedMessages, 'baton_pass');
        }, 1000); // Small delay for natural flow
      }
      
      return updatedMessages;
          } catch (error) {
        console.error(`üö® QUEUE ERROR: Error in ${stakeholder.name} response:`, error);
        setDynamicFeedback(null);
        
        // Clean up conversation state on error
        console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} error cleanup - clearing currentSpeaking`);
        setCurrentSpeaking(null);
        setConversationQueue(prev => {
          const newQueue = prev.filter(id => id !== stakeholder.id);
          console.log(`üöÄ QUEUE DEBUG: ${stakeholder.name} error cleanup - removed from queue. New queue: [${newQueue.join(', ')}]`);
          return newQueue;
        });
      
      // Force cleanup of thinking state on error
      setTimeout(() => {
        setDynamicFeedback(null);
      }, 100);
      
      throw error;
    }
  };

  // EXACT COPY of playMessageAudio from transcript meeting
  const playMessageAudio = async (messageId: string, text: string, stakeholder: any, autoPlay: boolean = true): Promise<void> => {
    console.log('Audio playback attempt:', { messageId, stakeholder: stakeholder.name, globalAudioEnabled, autoPlay });
    
    if (!globalAudioEnabled) {
      console.log('Audio disabled globally');
      return Promise.resolve();
    }

    try {
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.currentTime = 0;
        setCurrentAudio(null);
      }
      
      if (!autoPlay) {
        return Promise.resolve();
      }

      setCurrentSpeaker(stakeholder);
      setIsAudioPlaying(true);

      const voiceName = stakeholder.voice;
      console.log('üéµ Using voice:', voiceName, 'for stakeholder:', stakeholder.name);
      console.log('üîß Murf TTS Available:', murfTTS.isConfigured());
      
      if (murfTTS.isConfigured()) {
        console.log('‚úÖ Using Murf TTS for audio synthesis');
        const audioBlob = await murfTTS.synthesizeSpeech(text, stakeholder.name);
        
        if (audioBlob) {
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
        
        setCurrentAudio(audio);
        setPlayingMessageId(messageId);
        setAudioStates(prev => ({ ...prev, [messageId]: 'playing' }));
        
        return new Promise((resolve) => {
          audio.onended = () => {
            URL.revokeObjectURL(audioUrl);
            setCurrentAudio(null);
            setPlayingMessageId(null);
            setAudioStates(prev => ({ ...prev, [messageId]: 'stopped' }));
            setCurrentSpeaker(null);
            setIsAudioPlaying(false);
            console.log(`üöÄ AUDIO DEBUG: ${stakeholder.name} audio naturally ended`);
            resolve();
          };
          
          audio.onerror = (error) => {
            console.error('Audio element error:', error);
            URL.revokeObjectURL(audioUrl);
            setCurrentAudio(null);
            setPlayingMessageId(null);
            setAudioStates(prev => ({ ...prev, [messageId]: 'stopped' }));
            setCurrentSpeaker(null);
            setIsAudioPlaying(false);
            resolve();
          };
          
          audio.play().then(() => {
            console.log(`üöÄ AUDIO DEBUG: ${stakeholder.name} audio started playing`);
          }).catch((playError) => {
            console.error('Audio play error:', playError);
            URL.revokeObjectURL(audioUrl);
            setCurrentAudio(null);
            setPlayingMessageId(null);
            setAudioStates(prev => ({ ...prev, [messageId]: 'stopped' }));
            setCurrentSpeaker(null);
            setIsAudioPlaying(false);
            resolve();
          });
        });
        } else {
          console.warn('‚ùå Murf TTS returned null, falling back to browser TTS');
          setPlayingMessageId(messageId);
          setAudioStates(prev => ({ ...prev, [messageId]: 'playing' }));
          
          await playBrowserTTS(text);
          
          return new Promise((resolve) => {
            const estimatedDuration = Math.max(2000, text.length * 50);
            setTimeout(() => {
              setPlayingMessageId(null);
              setAudioStates(prev => ({ ...prev, [messageId]: 'stopped' }));
              setCurrentSpeaker(null);
              setIsAudioPlaying(false);
              resolve();
            }, estimatedDuration);
          });
        }
      } else {
        console.log('‚ö†Ô∏è Murf TTS not available or no voice, using browser TTS');
        setPlayingMessageId(messageId);
        setAudioStates(prev => ({ ...prev, [messageId]: 'playing' }));
        
        const browserAudio = await playBrowserTTS(text);
        
        return new Promise((resolve) => {
          // Browser TTS doesn't return a promise that waits for completion
          // So we'll use a timeout based on text length
          const estimatedDuration = Math.max(2000, text.length * 50); // ~50ms per character
          setTimeout(() => {
            setPlayingMessageId(null);
            setAudioStates(prev => ({ ...prev, [messageId]: 'stopped' }));
            setCurrentSpeaker(null);
            setIsAudioPlaying(false);
            console.log(`üöÄ AUDIO DEBUG: ${stakeholder.name} browser TTS estimated completion`);
            resolve();
          }, estimatedDuration);
        });
      }
    } catch (error) {
      console.error('Audio playback error:', error);
      setCurrentSpeaker(null);
      setIsAudioPlaying(false);
      setPlayingMessageId(null);
      return Promise.resolve();
    }
  };

  // EXACT COPY of transcript meeting's handleSendMessage - NO MODIFICATIONS
  const handleSendMessage = async () => {
    console.log('üéØ handleSendMessage called, inputMessage:', inputMessage);
    if (!inputMessage.trim()) {
      console.log('‚ùå handleSendMessage: Empty inputMessage, returning');
      return;
    }

    const messageContent = inputMessage.trim();
    setIsGeneratingResponse(true);
    setInputMessage('');

    const userMessage: Message = {
      id: `user-${Date.now()}`,
      speaker: 'user',
      content: messageContent,
      timestamp: new Date().toISOString()
    };

    let currentMessages = [...messages, userMessage];
    setMessages(currentMessages);

    // Add user message to background transcript (always captured)
    addToBackgroundTranscript(userMessage);

    try {
      // Check for direct stakeholder mentions in user message FIRST
      const aiService = AIService.getInstance();
      const availableStakeholders = selectedStakeholders.map(s => ({
        name: s.name,
        role: s.role,
        department: s.department,
        priorities: s.priorities,
        personality: s.personality,
        expertise: s.expertise || []
      }));

      // Get session context for smarter routing
      const lastSpeaker = currentMessages.length > 0 ? currentMessages[currentMessages.length - 1].stakeholderName : null;
      const conversationContext = currentMessages.slice(-3).map(m => `${m.stakeholderName || 'User'}: ${m.content.substring(0, 50)}`).join(' | ');
      
      // DYNAMIC AI-POWERED GREETING DETECTION (no hardcoded patterns)
      const isGeneralGreeting = await detectGeneralGreeting(messageContent);
      
      if (isGeneralGreeting) {
        console.log(`üëã GENERAL GREETING: "${messageContent}" detected by AI - all stakeholders respond with simple greetings`);
        
        // Show upcoming speakers in queue first
        const upcomingQueue = availableStakeholders.map(s => ({ name: s.name, id: s.id }));
        setResponseQueue(prev => ({ ...prev, upcoming: upcomingQueue }));
        console.log(`üìã QUEUE: Set upcoming speakers: ${upcomingQueue.map(s => s.name).join(', ')}`);
        
        // ULTRA-FAST GREETINGS: Generate all greetings in parallel, play sequentially
        console.log(`‚ö° ULTRA-FAST GREETINGS: Generating ${availableStakeholders.length} greetings in parallel`);
        
        // Generate all greetings and audio in parallel
        const greetingPromises = availableStakeholders.map(async (stakeholder, index) => {
          console.log(`üöÄ GREETING: ${stakeholder.name} - Starting parallel generation`);
          const startTime = performance.now();
          
          const simpleGreeting = await generateSimpleGreeting(stakeholder, messageContent);
          
          const aiTime = performance.now() - startTime;
          console.log(`üëã GREETING: ${stakeholder.name} greeting ready in ${aiTime.toFixed(0)}ms`);
          
          // Show text immediately
          const responseMessage = createResponseMessage(stakeholder, simpleGreeting, currentMessages.length + index);
          setMessages(prev => [...prev, responseMessage]);
          addToBackgroundTranscript(responseMessage);
          
          // Generate audio in background
          const audioPromise = globalAudioEnabled 
            ? murfTTS.synthesizeSpeech(simpleGreeting, stakeholder.name)
            : Promise.resolve(null);
          
          const totalTime = performance.now() - startTime;
          console.log(`‚ö° GREETING: ${stakeholder.name} text shown in ${totalTime.toFixed(0)}ms`);
          
          return {
            stakeholder,
            greeting: simpleGreeting,
            audioPromise
          };
        });
        
        // Wait for all greetings to be generated and shown
        const greetingResults = await Promise.all(greetingPromises);
        console.log(`üéØ ULTRA-FAST GREETINGS: All greeting texts shown, now playing audio...`);
        
        // Play audio sequentially to avoid overlap
        for (const result of greetingResults) {
          const audioBlob = await result.audioPromise;
          if (audioBlob) {
            console.log(`üéµ GREETING: Playing audio for ${result.stakeholder.name}`);
            setCurrentSpeaker(result.stakeholder);
            await murfTTS.playAudio(audioBlob);
            setCurrentSpeaker(null);
            console.log(`‚úÖ GREETING: ${result.stakeholder.name} finished speaking`);
          }
        }
        
        // Clear the queue when all done
        setResponseQueue(prev => ({ ...prev, upcoming: [] }));
        console.log(`üìã QUEUE: All greetings completed, queue cleared`);
        
        setIsGeneratingResponse(false);
        return;
      }

      // CHECK FOR EXPLICIT "EVERYONE" REQUESTS
      const explicitEveryoneRequest = await detectExplicitEveryoneRequest(messageContent);
      
      if (explicitEveryoneRequest) {
        console.log(`üë• EXPLICIT EVERYONE: User requested all stakeholders respond`);
        // Proceed to normal AI detection for multiple responses
      } else {
        // SMART SINGLE-STAKEHOLDER ROUTING: General questions get ONE relevant response
        const singleStakeholderResponse = await routeToSingleStakeholder(messageContent, availableStakeholders);
        
        if (singleStakeholderResponse) {
          console.log(`üéØ SMART ROUTING: ${singleStakeholderResponse.name} selected for general question`);
          
          // Single stakeholder responds to general question
          await handleFastMentionResponse([singleStakeholderResponse], messageContent, currentMessages);
          return;
        }
      }
      
      // FAST KEYWORD DETECTION: Skip expensive AI analysis for obvious mentions
      const fastMentionResult = detectStakeholderKeywords(messageContent, availableStakeholders);
      
      if (fastMentionResult.detected) {
        console.log(`üöÄ FAST DETECTION: Found ${fastMentionResult.stakeholders.map(s => s.name).join(', ')} via keywords`);
        
        // Use fast path for keyword-detected mentions
        await handleFastMentionResponse(fastMentionResult.stakeholders, messageContent, currentMessages);
        return;
      }
      
      // SMART SINGLE-STAKEHOLDER ROUTING: General questions go to most relevant person
      const primaryStakeholder = await routeQuestionToMostRelevantStakeholder(messageContent, availableStakeholders);
      
      if (primaryStakeholder) {
        console.log(`üéØ SMART ROUTING: "${messageContent}" ‚Üí ${primaryStakeholder.name} (most relevant)`);
        
        // Single stakeholder response
        await handleSingleStakeholderResponse(primaryStakeholder, messageContent, currentMessages);
        return;
      }
      
      // FALLBACK: Use AI detection only if smart routing fails
      console.log('üß† COMPLEX DETECTION: Using AI analysis for complex mention detection');
      const userMentionResult = await aiService.detectStakeholderMentions(
        messageContent, 
        availableStakeholders, 
        user?.id || 'anonymous',
        lastSpeaker,
        conversationContext
      );
      
      // Enhanced debugging for mention detection
      console.log('üîç DEBUG: User message analysis:', {
        messageContent,
        availableStakeholders: availableStakeholders.map(s => s.name),
        mentionResult: userMentionResult,
        threshold: AIService.getMentionConfidenceThreshold()
      });
      
      if (userMentionResult.mentionedStakeholders.length > 0 && userMentionResult.confidence >= AIService.getMentionConfidenceThreshold()) {
        const mentionedNames = userMentionResult.mentionedStakeholders.map(s => s.name).join(', ');
        console.log(`üéØ User directly mentioned stakeholder(s): ${mentionedNames} (${userMentionResult.mentionType}, confidence: ${userMentionResult.confidence})`);
        
        console.log(`üîç Detailed detection results:`, {
          totalDetected: userMentionResult.mentionedStakeholders.length,
          stakeholders: userMentionResult.mentionedStakeholders.map(s => ({ name: s.name, role: s.role, department: s.department })),
          availableStakeholders: selectedStakeholders.map(s => ({ id: s.id, name: s.name, role: s.role }))
        });
        
        // Show prominent feedback that stakeholders will respond
        const feedbackText = userMentionResult.mentionedStakeholders.length > 1 
          ? `üéØ ${mentionedNames} will respond shortly...`
          : `üéØ ${userMentionResult.mentionedStakeholders[0].name} will respond shortly...`;
        setDynamicFeedback(feedbackText);
        setTimeout(() => setDynamicFeedback(null), 2000);
        
        // Set up the response queue to show users what to expect
        const responseQueueData = userMentionResult.mentionedStakeholders.map(s => ({
          name: s.name,
          id: selectedStakeholders.find(st => st.name === s.name)?.id || 'unknown'
        }));
        
        setResponseQueue({
          current: responseQueueData[0]?.name || null,
          upcoming: responseQueueData.slice(1)
        });
        
        // Trigger all mentioned stakeholders to respond with parallel processing
        console.log(`üöÄ SIMPLE: Processing ${userMentionResult.mentionedStakeholders.length} stakeholders with SIMPLE approach`);
        
        // FASTEST APPROACH: Each stakeholder speaks immediately when ready (true streaming)
        console.log(`üöÄ FASTEST: Starting true streaming for ${userMentionResult.mentionedStakeholders.length} stakeholders`);
        
        let speakingQueue: any[] = [];
        let currentlySpeaking = false;
        
        // Function to process the speaking queue
        const processQueue = async () => {
          if (currentlySpeaking || speakingQueue.length === 0) return;
          
          currentlySpeaking = true;
          const next = speakingQueue.shift();
          
          console.log(`üéµ FASTEST: ${next.stakeholder.name} starting to speak`);
          if (next.audioBlob) {
            await murfTTS.playAudio(next.audioBlob);
          }
          console.log(`‚úÖ FASTEST: ${next.stakeholder.name} finished speaking`);
          
          currentlySpeaking = false;
          processQueue(); // Process next in queue
        };
        
        // ULTRA-FAST PARALLEL PROCESSING: AI + Audio generation in parallel
        console.log(`‚ö° ULTRA-FAST: Starting parallel generation for ${userMentionResult.mentionedStakeholders.length} stakeholders`);
        
        const stakeholderPromises = userMentionResult.mentionedStakeholders.map(async (mentionedStakeholder, index) => {
          const stakeholder = selectedStakeholders.find(s => s.name === mentionedStakeholder.name);
          if (!stakeholder) return null;
          
          console.log(`üöÄ ULTRA-FAST: ${stakeholder.name} - Starting parallel AI + Audio generation`);
          const startTime = performance.now();
          
          // PARALLEL GENERATION: AI and audio prep happen simultaneously
          const [response] = await Promise.all([
            // AI Generation
            generateStakeholderResponse(
              stakeholder,
              messageContent,
              currentMessages,
              { conversationPhase: 'as_is' },
              'direct_mention'
            ),
            // Placeholder for audio optimization - we'll generate audio immediately after AI completes
            Promise.resolve()
          ]);
          
          const aiTime = performance.now() - startTime;
          console.log(`üß† ULTRA-FAST: ${stakeholder.name} AI complete in ${aiTime.toFixed(0)}ms`);
          
          // Show text response IMMEDIATELY - don't wait for audio
          const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length + index);
          setMessages(prev => [...prev, responseMessage]);
          addToBackgroundTranscript(responseMessage);
          
          // Generate audio in background while user sees text
          const audioPromise = globalAudioEnabled && response 
            ? murfTTS.synthesizeSpeech(response, stakeholder.name)
            : Promise.resolve(null);
          
          const totalTime = performance.now() - startTime;
          console.log(`‚ö° ULTRA-FAST: ${stakeholder.name} text shown in ${totalTime.toFixed(0)}ms, audio generating...`);
          
          return {
            stakeholder,
            response,
            audioPromise,
            index
          };
        });
        
        // Process responses as they complete (streaming)
        const results = await Promise.all(stakeholderPromises);
        const validResults = results.filter(r => r !== null);
        
        console.log(`üéØ ULTRA-FAST: All text responses shown, now processing audio queue...`);
        
        // Play audio sequentially as it becomes ready
        for (const result of validResults) {
          const audioBlob = await result.audioPromise;
          if (audioBlob) {
            console.log(`üéµ ULTRA-FAST: Playing audio for ${result.stakeholder.name}`);
            setCurrentSpeaker(result.stakeholder);
            await murfTTS.playAudio(audioBlob);
            setCurrentSpeaker(null);
            console.log(`‚úÖ ULTRA-FAST: ${result.stakeholder.name} finished speaking`);
          }
        }
        
        console.log(`üèÅ ULTRA-FAST: All responses complete!`);
        
        // Clear the response queue when all responses are complete
        setResponseQueue({ current: null, upcoming: [] });
        console.log('üèÅ All direct mention responses complete - queue cleared');
        
        setIsGeneratingResponse(false);
        return; // Exit early - don't go through normal conversation flow
      }

      // If no direct mention detected, proceed with normal conversation flow
      // Dynamic conversation type detection
      const isGroup = isGroupMessage(messageContent);
      const isGreeting = isSimpleGreeting(messageContent);
      
      if (isGroup && isGreeting) {
        await handleAdaptiveGreeting(messageContent, currentMessages);
      } else {
        // FAST PATH: Handle general questions with smart stakeholder selection
        console.log('üöÄ FAST PATH: Using optimized response for general question');
        await handleFastResponse(messageContent, currentMessages);
      }
    } catch (error) {
      console.error('Error generating AI response:', error);
    } finally {
      setIsGeneratingResponse(false);
    }
  };

  // FAST RESPONSE SYSTEM: Smart stakeholder selection and optimized generation
  const getQuickStakeholder = (message: string) => {
    const msg = message.toLowerCase();
    
    // Technical/IT questions ‚Üí David Thompson
    if (msg.includes('system') || msg.includes('technical') || msg.includes('security') || 
        msg.includes('database') || msg.includes('server') || msg.includes('bug') ||
        msg.includes('integration') || msg.includes('api') || msg.includes('performance')) {
      console.log('üéØ FAST: Routing technical question to David Thompson');
      return selectedStakeholders.find(s => s.name === 'David Thompson');
    }
    
    // Customer/Service questions ‚Üí Aisha Ahmed  
    if (msg.includes('customer') || msg.includes('user') || msg.includes('feedback') ||
        msg.includes('support') || msg.includes('service') || msg.includes('complaint') ||
        msg.includes('satisfaction') || msg.includes('experience') || msg.includes('help')) {
      console.log('üéØ FAST: Routing customer question to Aisha Ahmed');
      return selectedStakeholders.find(s => s.name === 'Aisha Ahmed');
    }
    
    // Planning/Operations questions ‚Üí James Walker
    if (msg.includes('timeline') || msg.includes('plan') || msg.includes('schedule') ||
        msg.includes('deadline') || msg.includes('project') || msg.includes('process') ||
        msg.includes('workflow') || msg.includes('status') || msg.includes('progress')) {
      console.log('üéØ FAST: Routing planning question to James Walker');
      return selectedStakeholders.find(s => s.name === 'James Walker');
    }
    
    // Default: Aisha (tends to give quicker, more direct responses)
    console.log('üéØ FAST: Using default stakeholder Aisha Ahmed');
    return selectedStakeholders.find(s => s.name === 'Aisha Ahmed') || selectedStakeholders[0];
  };

    // FAST KEYWORD DETECTION: Skip expensive AI analysis for obvious mentions
  const detectStakeholderKeywords = (message: string, availableStakeholders: any[]) => {
    const msg = message.toLowerCase();
    const detected: any[] = [];
    
    // Check for each stakeholder's name variations
    for (const stakeholder of availableStakeholders) {
      const name = stakeholder.name.toLowerCase();
      const firstName = name.split(' ')[0]; // "david" from "David Thompson"
      const lastName = name.split(' ')[1]; // "thompson" from "David Thompson"
      
      // Check various mention patterns
      if (msg.includes(firstName) || 
          msg.includes(lastName) || 
          msg.includes(name) ||
          msg.includes(`@${firstName}`) ||
          msg.includes(`hey ${firstName}`) ||
          msg.includes(`hi ${firstName}`)) {
        detected.push(stakeholder);
        console.log(`üéØ KEYWORD: Found ${stakeholder.name} via "${firstName}" or "${lastName}"`);
      }
    }
    
    return {
      detected: detected.length > 0,
      stakeholders: detected
    };
  };

  // INTELLIGENT RESPONSE LENGTH: Match response length to question complexity
  const getResponseStyle = (message: string) => {
    const msg = message.toLowerCase();
    
          // BRIEF responses (1-2 sentences) - Simple pleasantries only
      if (msg.includes('hi') || msg.includes('hello') || msg.includes('hey') ||
          msg.includes("what's up") || msg.includes('whats up') ||
          msg.includes('how are you') || msg.includes('how you doing') ||
          msg.includes('good morning') || msg.includes('good afternoon') ||
          (msg.includes('hi') && msg.includes('guys')) || 
          (msg.includes('hello') && msg.includes('everyone'))) {
        return 'greeting';
      }
    
    // DETAILED responses (multiple sentences, explanations)
    if (msg.includes('explain') || msg.includes('walk me through') || 
        msg.includes('tell me about') || msg.includes('how does') ||
        msg.includes('what is the process') || msg.includes('step by step') ||
        msg.includes('current process') || msg.includes('workflow')) {
      return 'detailed';
    }
    
    // MEDIUM responses (2-3 sentences)
    return 'medium';
  };

  // DYNAMIC RESPONSE CACHING: Cache AI-generated responses for performance (no hardcoding)
  const responseCache = new Map<string, { response: string, timestamp: number }>();
  const CACHE_DURATION = 5 * 60 * 1000; // 5 minutes
  
  // Clear greeting cache on component mount to remove repetitive cached responses
  useEffect(() => {
    console.log('üßπ CACHE: Clearing all cached responses to apply new expert-level intelligence');
    // Clear ALL cached responses to ensure new Silicon Valley expert prompts take effect
    responseCache.clear();
    console.log('üóëÔ∏è CACHE: All cached responses cleared for expert upgrade');
  }, []); // Only run on mount
  
  const getCachedResponse = (message: string, stakeholder: any): string | null => {
    const msg = message.toLowerCase().trim();
    
    // Check regular dynamic cache (including greetings)
    // No hardcoded responses - everything should be AI generated and cached
    
    // Regular caching for other responses
    const cacheKey = `${stakeholder.name}-${msg}`;
    const cached = responseCache.get(cacheKey);
    
    if (cached && (Date.now() - cached.timestamp) < CACHE_DURATION) {
      console.log(`üöÄ CACHE HIT: Using cached dynamic response for ${stakeholder.name}`);
      return cached.response;
    }
    
    return null; // No valid cache, generate fresh
  };
  
  const setCachedResponse = (message: string, stakeholder: any, response: string) => {
    const cacheKey = `${stakeholder.name}-${message.toLowerCase().trim()}`;
    responseCache.set(cacheKey, { response, timestamp: Date.now() });
    console.log(`üíæ CACHED: Stored dynamic response for ${stakeholder.name}`);
  };

  const handleFastResponse = async (messageContent: string, currentMessages: Message[]) => {
    console.log('‚ö° FAST: Starting optimized response generation');
    
    // 1. Get smart stakeholder selection (near-instant)
    const stakeholder = getQuickStakeholder(messageContent);
    if (!stakeholder) {
      console.error('‚ùå FAST: No stakeholder found');
      return;
    }
    
    console.log(`‚ö° FAST: Selected ${stakeholder.name} for response`);
    
    // 2. Check for cached response first (instant!)
    const cachedResponse = getCachedResponse(messageContent, stakeholder);
    
    if (cachedResponse) {
      console.log(`üöÄ CACHE HIT: Using cached dynamic response for ${stakeholder.name}`);
      
      // 3. Show response immediately in transcript (instant feedback!)
      const responseMessage = createResponseMessage(stakeholder, cachedResponse, currentMessages.length);
      setMessages(prev => [...prev, responseMessage]);
      addToBackgroundTranscript(responseMessage);
      
      // 4. Generate and play audio in parallel (user already sees text)
      if (globalAudioEnabled) {
        console.log(`üéµ CACHE: Generating audio for ${stakeholder.name} (background)`);
        const audioBlob = await murfTTS.synthesizeSpeech(cachedResponse, stakeholder.name);
        if (audioBlob) {
          await murfTTS.playAudio(audioBlob);
          console.log(`‚úÖ CACHE: ${stakeholder.name} finished speaking`);
        }
      }
      return;
    }
    
    // 3. No cache hit - generate fresh response (but show progress)
    console.log(`üß† GENERATING: Creating fresh response for ${stakeholder.name}`);
    
    // Show "thinking" placeholder immediately
    const thinkingMessage = createResponseMessage(stakeholder, `${stakeholder.name} is thinking...`, currentMessages.length);
    setMessages(prev => [...prev, thinkingMessage]);
    
    try {
      // Generate response with intelligent length control
      const response = await generateIntelligentStakeholderResponse(
        stakeholder,
        messageContent,
        currentMessages,
        getResponseStyle(messageContent)
      );
      
      console.log(`‚úÖ GENERATED: AI response ready for ${stakeholder.name}, caching and updating transcript...`);
      
      // Cache the generated response for future use
      setCachedResponse(messageContent, stakeholder, response);
      
      // Replace thinking message with actual response
      const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length);
      setMessages(prev => prev.map(msg => 
        msg.id === thinkingMessage.id ? responseMessage : msg
      ));
      addToBackgroundTranscript(responseMessage);
      
      // 4. Generate and play audio immediately
      if (globalAudioEnabled && response) {
        console.log(`üéµ FAST: Generating and playing audio for ${stakeholder.name}`);
        const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
        if (audioBlob) {
          await murfTTS.playAudio(audioBlob);
          console.log(`‚úÖ FAST: ${stakeholder.name} finished speaking`);
        } else {
          console.warn('‚ö†Ô∏è FAST: Audio generation failed, response added to transcript only');
        }
      }
      
    } catch (error) {
      console.error('‚ùå FAST: Error in fast response generation:', error);
    }
  };

  // FAST MENTION RESPONSE: Optimized for when stakeholders are directly mentioned
  const handleFastMentionResponse = async (mentionedStakeholders: any[], messageContent: string, currentMessages: Message[]) => {
    console.log(`üöÄ FAST MENTION: Processing ${mentionedStakeholders.length} mentioned stakeholders`);
    
    // Process each mentioned stakeholder with optimized approach
    for (const stakeholder of mentionedStakeholders) {
      console.log(`‚ö° FAST MENTION: Generating response for ${stakeholder.name}`);
      
      try {
              // Check cache first (even for mentions)
      const cachedResponse = getCachedResponse(messageContent, stakeholder);
      
      if (cachedResponse) {
        console.log(`üöÄ CACHE HIT: Using cached dynamic response for mentioned ${stakeholder.name}`);
        
        // Show response immediately
        const responseMessage = createResponseMessage(stakeholder, cachedResponse, currentMessages.length);
        setMessages(prev => [...prev, responseMessage]);
        addToBackgroundTranscript(responseMessage);
        
        // Generate and play audio
        if (globalAudioEnabled) {
          const audioBlob = await murfTTS.synthesizeSpeech(cachedResponse, stakeholder.name);
          if (audioBlob) {
            await murfTTS.playAudio(audioBlob);
            console.log(`‚úÖ FAST MENTION: ${stakeholder.name} finished speaking (cached)`);
          }
        }
      } else if (getResponseStyle(messageContent) === 'greeting') {
        // SPECIAL FAST PATH FOR GREETINGS: Use minimal prompt
        console.log(`üëã SIMPLE GREETING: Generating basic greeting for ${stakeholder.name}`);
        
        const response = await generateSimpleGreeting(stakeholder, messageContent);
        
        // Cache the simple greeting
        setCachedResponse(messageContent, stakeholder, response);
        
        // Show response immediately
        const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length);
        setMessages(prev => [...prev, responseMessage]);
        addToBackgroundTranscript(responseMessage);
        
        // Generate and play audio
        if (globalAudioEnabled) {
          const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
          if (audioBlob) {
            await murfTTS.playAudio(audioBlob);
            console.log(`‚úÖ SIMPLE GREETING: ${stakeholder.name} finished speaking`);
          }
        }
      } else {
          // Generate with STREAMLINED prompt for faster AI response
          console.log(`üß† STREAMLINED: Generating fast response for ${stakeholder.name}`);
          
          // Show thinking indicator
          const thinkingMessage = createResponseMessage(stakeholder, `${stakeholder.name} is responding...`, currentMessages.length);
          setMessages(prev => [...prev, thinkingMessage]);
          
          // Use intelligent generation with length control
          const response = await generateIntelligentStakeholderResponse(
            stakeholder,
            messageContent,
            currentMessages,
            getResponseStyle(messageContent)
          );
          
          console.log(`‚úÖ INTELLIGENT: Response ready for ${stakeholder.name}, caching...`);
          
          // Cache the generated response
          setCachedResponse(messageContent, stakeholder, response);
          
          // Replace thinking message with actual response
          const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length);
          setMessages(prev => prev.map(msg => 
            msg.id === thinkingMessage.id ? responseMessage : msg
          ));
          addToBackgroundTranscript(responseMessage);
          
          // Generate and play audio
          if (globalAudioEnabled) {
            const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
            if (audioBlob) {
              await murfTTS.playAudio(audioBlob);
              console.log(`‚úÖ FAST MENTION: ${stakeholder.name} finished speaking`);
            }
          }
        }
        
      } catch (error) {
        console.error(`‚ùå FAST MENTION: Error processing ${stakeholder.name}:`, error);
      }
    }
  };

  // SILICON VALLEY EXPERT AI GENERATION: Ultra-intelligent responses
  const generateIntelligentStakeholderResponse = async (
    stakeholder: any, 
    messageContent: string, 
    currentMessages: Message[], 
    responseStyle: string
  ): Promise<string> => {
    console.log(`üß† SILICON VALLEY EXPERT: Generating ${responseStyle} response for ${stakeholder.name}`);
    
    try {
      // Build expert-level context
      const expertContext = {
        conversationPhase: 'expert_analysis' as const,
        conversationHistory: currentMessages.slice(-3),
        projectContext: {
          name: selectedProject?.name || 'Current Project',
          phase: 'active'
        }
      };
      
      // EXPERT-LEVEL RESPONSE INSTRUCTIONS - NO GENERIC RESPONSES
      const expertInstructions = {
        greeting: "You're a Silicon Valley expert. Respond with a brief, confident greeting that hints at your expertise. NO generic pleasantries.",
        brief: "Give a sharp, expert-level insight in 1-2 sentences. Show your deep domain knowledge immediately. Reference specific systems, metrics, or pain points.",
        medium: "Provide expert analysis in 2-3 sentences. Include specific numbers, systems, or business impact. Make it clear you're the authority in your domain.",
        detailed: "Give comprehensive expert analysis with specific metrics, system details, and business impact. Include numbers, timeframes, and technical specifics that only a domain expert would know."
      };
      
      // Detect domain-specific questions
      const isDomainQuestion = messageContent.toLowerCase().includes('finance') || 
                              messageContent.toLowerCase().includes('operations') ||
                              messageContent.toLowerCase().includes('technical') ||
                              messageContent.toLowerCase().includes('systems') ||
                              messageContent.toLowerCase().includes('issues') ||
                              messageContent.toLowerCase().includes('problems');
      
      // SILICON VALLEY EXPERT BEHAVIOR
      const expertBehavior = `\n\nüß† SILICON VALLEY EXPERT MODE:
- You're a $500K+ expert who thinks 10 steps ahead
- Jump IMMEDIATELY into domain-specific analysis
- Reference specific metrics, systems, and business impact
- Show deep technical/business knowledge
- NO generic responses or introductions
- Be the smartest person in the room for your domain
- Think like a top-tier consultant

DOMAIN EXPERTISE: ${stakeholder.expertise?.join(', ') || stakeholder.role}
YOUR AUTHORITY: ${stakeholder.role} - you KNOW this inside and out`;

      // Add specific domain guidance
      const domainGuidance = isDomainQuestion ? 
        "\n\nDOMAIN FOCUS: Dive deep into your domain expertise. Reference specific systems, processes, metrics, pain points, and business impact. Show why you're worth $500K+ in Silicon Valley." : 
        "";
      
      // Generate expert-level response
      const response = await generateStakeholderResponse(
        stakeholder,
        `${messageContent}\n\nEXPERT LEVEL: ${expertInstructions[responseStyle]}${expertBehavior}${domainGuidance}`,
        expertContext,
        'direct_mention'
      );
      
      return response;
      
    } catch (error) {
      console.error('‚ùå EXPERT: AI generation failed, using domain-specific fallback');
      
      // Domain-specific expert fallback (no generic responses)
      const expertFallback = stakeholder.role.toLowerCase().includes('finance') ? 
        "We're seeing critical cash flow visibility issues and manual approval bottlenecks." :
        stakeholder.role.toLowerCase().includes('operations') ? 
        "Our biggest operational challenge is the handoff delays between systems." :
        stakeholder.role.toLowerCase().includes('it') ? 
        "From a technical perspective, we're hitting scalability and integration issues." :
        `Looking at this from a ${stakeholder.department || stakeholder.role} perspective, we need to dive deeper into the specifics.`;
      
      return expertFallback;
    }
  };

  // EXPERT GREETING GENERATION: Professional, confident greetings
  const generateSimpleGreeting = async (stakeholder: any, messageContent: string): Promise<string> => {
    console.log(`üëã EXPERT GREETING: Generating professional greeting for ${stakeholder.name}`);
    
    try {
      // Direct OpenAI call with expert-level context
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${import.meta.env.VITE_OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4',
          messages: [
            {
              role: 'system',
              content: `You are ${stakeholder.name.split(' ')[0]}, a ${stakeholder.role} with Silicon Valley-level expertise. Respond to this greeting with a brief, confident, professional greeting. Maximum 6 words. Examples: "Hey team!", "Morning everyone!", "Good to see you!", "Hey! Ready to dive in!", "Hi! Let's get started!", "Hello! Great timing!". Show confidence and readiness. NO work details, just professional energy.`
            },
            {
              role: 'user',
              content: messageContent
            }
          ],
          max_tokens: 15,
          temperature: 0.8
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      let greeting = data.choices?.[0]?.message?.content?.trim() || "Hey team!";
      
      // Extra safety: ensure it's professional but brief
      greeting = greeting.replace(/[.!?]+$/, ''); // Remove ending punctuation
      greeting = greeting.split(' ').slice(0, 6).join(' '); // Max 6 words
      if (!greeting.endsWith('!')) greeting += '!'; // Add confident exclamation
      
      console.log(`‚úÖ EXPERT GREETING: Generated "${greeting}" for ${stakeholder.name}`);
      return greeting;
      
    } catch (error) {
      console.error('‚ùå EXPERT GREETING: Generation failed, using professional fallback', error);
      // Professional expert fallbacks based on role
      const expertFallbacks = stakeholder.role.toLowerCase().includes('finance') ? [
        "Hey team!", "Morning everyone!", "Ready to dive in!", "Good to see you!", "Let's get started!"
      ] : stakeholder.role.toLowerCase().includes('operations') ? [
        "Hey everyone!", "Morning team!", "Ready to go!", "Good timing!", "Let's do this!"
      ] : stakeholder.role.toLowerCase().includes('it') ? [
        "Hey folks!", "Morning all!", "Systems ready!", "Good to connect!", "Let's get technical!"
      ] : [
        "Hey team!", "Morning everyone!", "Good to see you!", "Ready when you are!", "Let's get started!"
      ];
      
      const fallbackIndex = Math.floor(Math.random() * expertFallbacks.length);
      return expertFallbacks[fallbackIndex];
    }
  };

  const handleVoiceInput = (text: string) => {
    setInputMessage(text);
    if (inputRef.current) {
      inputRef.current.focus();
    }
  };

  const handleTranscribingChange = (transcribing: boolean) => {
    setIsTranscribing(transcribing);
  };

  // ENHANCED: Streaming voice input functions with better user feedback
  const startStreamingVoiceInput = async () => {
    if (isListening) {
      // Stop current session
      await stopStreamingVoiceInput();
      return;
    }

    try {
      console.log('üé§ Starting streaming voice input...');
      setIsListening(true);
      setLiveTranscript('');
      setFinalTranscript('');
      setDynamicFeedback('üé§ Starting voice input... Please allow microphone access if prompted');
      
      const streaming = createDeepgramStreaming({
        onTranscript: (transcript: string, isFinal: boolean) => {
          console.log(`üìù Live transcript (${isFinal ? 'FINAL' : 'INTERIM'}): "${transcript}"`);
          
          // Clear the starting feedback once we get transcripts
          if (dynamicFeedback?.includes('Starting voice input')) {
            setDynamicFeedback('üé§ Listening... Keep speaking');
          }
          
          if (isFinal) {
            // Accumulate final transcripts
            setFinalTranscript(prev => {
              const newFinal = prev ? `${prev} ${transcript}`.trim() : transcript.trim();
              console.log(`üìù Accumulated final transcript: "${newFinal}"`);
              return newFinal;
            });
            setLiveTranscript(''); // Clear interim
            setDynamicFeedback('üé§ Got it! Continue speaking or click to finish');
          } else {
            // Show interim transcript
            setLiveTranscript(transcript);
            setDynamicFeedback('üé§ Listening... (auto-stops after 2.5 seconds of silence)');
          }
        },
        
        onOpen: () => {
          console.log('üîå DEEPGRAM: Connection established');
          setDynamicFeedback('üé§ Ready! Start speaking...');
        },
        
        onSilenceDetected: () => {
          console.log('üîá Silence detected, auto-stopping...');
          setDynamicFeedback('üîÑ Processing your message...');
          // Process any accumulated transcript before stopping
          setTimeout(() => {
            stopStreamingVoiceInput();
          }, 100); // Small delay to catch any final transcripts
        },
        
        onError: (error: Error) => {
          console.error('‚ùå Streaming error:', error);
          setIsListening(false);
          setLiveTranscript('');
          
          // Show user-friendly error message
          setDynamicFeedback(`‚ùå Voice input failed: ${error.message}`);
          setTimeout(() => setDynamicFeedback(null), 5000);
        },
        
        onClose: () => {
          console.log('üîå Streaming session closed');
          setIsListening(false);
        }
      });
      
      setStreamingService(streaming);
      await streaming.startStreaming();
      
      console.log('‚úÖ Streaming voice input started');
      
    } catch (error) {
      console.error('‚ùå Failed to start streaming voice input:', error);
      setIsListening(false);
      setLiveTranscript('');
      
      // Show specific error messages
      let errorMessage = '‚ùå Voice input failed: ';
      if (error instanceof Error) {
        if (error.message.includes('permission')) {
          errorMessage += 'Microphone permission denied. Please allow microphone access.';
        } else if (error.message.includes('not found')) {
          errorMessage += 'No microphone found. Please check your audio devices.';
        } else if (error.message.includes('not configured')) {
          errorMessage += 'Voice service not available. Please try typing your message.';
        } else {
          errorMessage += error.message;
        }
      } else {
        errorMessage += 'Please try again or type your message.';
      }
      
      setDynamicFeedback(errorMessage);
      setTimeout(() => setDynamicFeedback(null), 8000);
    }
  };

  const stopStreamingVoiceInput = async () => {
    if (!streamingService || !isListening) return;
    
    try {
      console.log('üõë Stopping streaming voice input...');
      await streamingService.stopStreaming();
      setStreamingService(null);
      setIsListening(false);
      
      // Process any accumulated final transcript
      const transcriptToProcess = finalTranscript.trim() || liveTranscript.trim();
      if (transcriptToProcess) {
        console.log('üöÄ Processing transcript on stop:', transcriptToProcess);
        handleFinalTranscript(transcriptToProcess);
      } else {
        console.log('‚ö†Ô∏è No transcript to process');
      }
      
      setLiveTranscript('');
      setFinalTranscript('');
      
    } catch (error) {
      console.error('‚ùå Error stopping streaming:', error);
      setIsListening(false);
      setLiveTranscript('');
    }
  };

  const handleFinalTranscript = async (transcript: string) => {
    if (!transcript.trim()) return;
    
    console.log('üöÄ Auto-processing final transcript:', transcript);
    
    // Create user message
    const userMessage: Message = {
      id: `user-${Date.now()}`,
      speaker: 'user',
      content: transcript,
      timestamp: new Date().toISOString(),
      stakeholderName: 'User'
    };
    
    // Add to messages and transcript
    const updatedMessages = [...messages, userMessage];
    setMessages(updatedMessages);
    addToBackgroundTranscript(userMessage);
    
    // Auto-trigger AI response using existing mention detection logic
    try {
      const aiService = AIService.getInstance();
      const availableStakeholders = selectedStakeholders.map(s => ({
        name: s.name,
        role: s.role,
        department: s.department,
        priorities: s.priorities,
        personality: s.personality,
        expertise: s.expertise || []
      }));

      // Get session context for smarter routing  
      const lastSpeaker = messages.length > 0 ? messages[messages.length - 1].stakeholderName : null;
      const conversationContext = messages.slice(-3).map(m => `${m.stakeholderName || 'User'}: ${m.content.substring(0, 50)}`).join(' | ');

      const userMentionResult = await aiService.detectStakeholderMentions(
        transcript, 
        availableStakeholders, 
        user?.id || 'anonymous',
        lastSpeaker,
        conversationContext
      );
      
      console.log('üîç Auto-processing: User message analysis:', {
        transcript,
        mentionResult: userMentionResult
      });
      
      if (userMentionResult.mentionedStakeholders.length > 0 && userMentionResult.confidence >= AIService.getMentionConfidenceThreshold()) {
        // Process mentioned stakeholders in parallel
        await processStakeholdersInParallel(
          userMentionResult.mentionedStakeholders, 
          transcript, 
          updatedMessages, 
          'direct_mention'
        );
      } else {
        // General conversation - pick random stakeholder
        const randomStakeholder = selectedStakeholders[Math.floor(Math.random() * selectedStakeholders.length)];
        console.log(`üéØ Auto-processing: ${randomStakeholder.name} responding to general message`);
        await processStakeholdersInParallel(
          [{ name: randomStakeholder.name }], 
          transcript, 
          updatedMessages, 
          'general_question'
        );
      }
    } catch (error) {
      console.error('‚ùå Error in auto-processing:', error);
    }
  };

  // Legacy: Direct voice recording functions (keeping for fallback)
  const startDirectRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      
      // Use Deepgram-optimized audio format
      const formats = getSupportedDeepgramFormats();
      const options = { mimeType: formats.preferredMimeType };
      
      // Fallback to supported format if preferred is not available
      let mediaRecorder;
      try {
        mediaRecorder = new MediaRecorder(stream, options);
      } catch (e) {
        console.log('üéôÔ∏è Preferred format not supported, using default');
        mediaRecorder = new MediaRecorder(stream);
      }
      
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        console.log('üé¨ MediaRecorder stopped, processing audio...');
        const audioBlob = new Blob(audioChunksRef.current, { type: getSupportedDeepgramFormats().preferredMimeType });
        console.log('üì¶ Audio blob created:', { size: audioBlob.size, type: audioBlob.type });
        await transcribeAndSend(audioBlob);
        
        // Clean up
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
          streamRef.current = null;
        }
      };

      mediaRecorder.start();
      setIsRecording(true);
      setIsTranscribing(false); // Only set to true when actually transcribing
      setDynamicFeedback('üé§ Recording your message... Click microphone to stop');
    } catch (error) {
      console.error('Error starting recording:', error);
      setIsRecording(false);
      setIsTranscribing(false);
      
      // Show user-friendly error message
      let errorMessage = '‚ùå Recording failed. ';
      if (error instanceof Error) {
        if (error.name === 'NotAllowedError') {
          errorMessage += 'Microphone permission denied. Please allow microphone access.';
        } else if (error.name === 'NotFoundError') {
          errorMessage += 'No microphone found. Please check your audio devices.';
        } else if (error.name === 'NotSupportedError') {
          errorMessage += 'Recording not supported in this browser.';
        } else {
          errorMessage += error.message;
        }
      } else {
        errorMessage += 'Please try again.';
      }
      
      setDynamicFeedback(errorMessage);
      setTimeout(() => setDynamicFeedback(null), 5000);
    }
  };

  const stopDirectRecording = () => {
    console.log('üõë Stop recording clicked');
    if (mediaRecorderRef.current && isRecording) {
      console.log('üéôÔ∏è Stopping MediaRecorder...');
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      setDynamicFeedback('üîÑ Processing and transcribing your message...');
      // Safety timeout to clear feedback in case something goes wrong
      setTimeout(() => setDynamicFeedback(null), 10000);
    } else {
      console.log('‚ö†Ô∏è MediaRecorder not available or not recording');
    }
  };

  const transcribeAndSend = async (audioBlob: Blob) => {
    try {
      setIsTranscribing(true);
      
      // Check if OpenAI is configured
      const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
      if (!apiKey || apiKey === 'your-openai-api-key-here') {
        // Test mode - simulate transcription
        console.log('üß™ Test mode: Simulating transcription');
        await new Promise(resolve => setTimeout(resolve, 2000)); // Simulate processing time
        
        const testTranscription = "Hello, this is a test message from the voice recorder.";
        setDynamicFeedback('üß™ Test mode: Simulated transcription');
        
        // Send the test message
        setInputMessage(testTranscription);
        await handleSendMessageWithText(testTranscription);
        
        // Clear feedback after message is fully processed
        setDynamicFeedback(null);
        return;
      }
      
              const transcription = await transcribeWithDeepgram(audioBlob);
      
      if (transcription && transcription.trim()) {
        console.log('üöÄ Auto-sending transcribed message:', transcription);
        // Automatically send the transcription
        await handleSendMessageWithText(transcription);
        
        // Clear feedback after message is fully processed
        setDynamicFeedback(null);
      } else {
        console.warn('No transcription received or transcription was empty');
        setDynamicFeedback('‚ùå No speech detected. Please try again.');
        setTimeout(() => setDynamicFeedback(null), 3000);
      }
    } catch (error) {
      console.error('Error transcribing audio:', error);
      
      // Show user-friendly error message
      let errorMessage = '‚ùå Transcription failed. ';
      if (error instanceof Error) {
        if (error.message.includes('API key')) {
          errorMessage += 'OpenAI API key not configured.';
        } else if (error.message.includes('quota')) {
          errorMessage += 'API quota exceeded.';
        } else if (error.message.includes('network')) {
          errorMessage += 'Network error. Check your connection.';
        } else {
          errorMessage += 'Please try again.';
        }
      } else {
        errorMessage += 'Please try again.';
      }
      
      setDynamicFeedback(errorMessage);
      setTimeout(() => setDynamicFeedback(null), 5000);
    } finally {
      setIsTranscribing(false);
    }
  };

  const handleSendMessageWithText = async (messageText: string) => {
    console.log('üì§ handleSendMessageWithText called with:', messageText);
    if (!messageText.trim()) {
      console.log('‚ùå Empty message text, returning');
      return;
    }
    
    // Direct send without relying on inputMessage state
    console.log('üöÄ Sending message directly...');
    const messageContent = messageText.trim();
    setIsGeneratingResponse(true);
    setInputMessage(''); // Clear input

    const userMessage: Message = {
      id: `user-${Date.now()}`,
      speaker: 'user',
      content: messageContent,
      timestamp: new Date().toISOString()
    };

    console.log('üìù Adding user message to transcript:', userMessage);
    setMessages(prev => [...prev, userMessage]);

    // Process AI responses - copy exact logic from handleSendMessage
    console.log('ü§ñ Processing AI responses...');
    
    let currentMessages = [...messages, userMessage];
    
    // Add user message to background transcript (always captured)
    addToBackgroundTranscript(userMessage);

    try {
      // Check for direct stakeholder mentions in user message FIRST
      const aiService = AIService.getInstance();
      const availableStakeholders = selectedStakeholders.map(s => ({
        name: s.name,
        role: s.role,
        department: s.department,
        priorities: s.priorities,
        personality: s.personality,
        expertise: s.expertise || []
      }));

      // Get session context for smarter routing
      const lastSpeaker = currentMessages.length > 0 ? currentMessages[currentMessages.length - 1].stakeholderName : null;
      const conversationContext = currentMessages.slice(-3).map(m => `${m.stakeholderName || 'User'}: ${m.content.substring(0, 50)}`).join(' | ');
      
      const userMentionResult = await aiService.detectStakeholderMentions(
        messageContent, 
        availableStakeholders, 
        user?.id || 'anonymous',
        lastSpeaker,
        conversationContext
      );
      
      if (userMentionResult.mentionedStakeholders.length > 0 && userMentionResult.confidence >= AIService.getMentionConfidenceThreshold()) {
        const mentionedNames = userMentionResult.mentionedStakeholders.map(s => s.name).join(', ');
        console.log(`üéØ User directly mentioned stakeholder(s): ${mentionedNames}`);
        
        // Use fast single response for one stakeholder, parallel for multiple
        if (userMentionResult.mentionedStakeholders.length === 1) {
          const stakeholder = selectedStakeholders.find(s => s.name === userMentionResult.mentionedStakeholders[0].name);
          if (stakeholder) {
            await processSingleStakeholderStreaming(stakeholder, messageContent, currentMessages, 'direct_mention');
          }
        } else {
          // Use parallel processing only for multiple stakeholders
          await processStakeholdersInParallel(userMentionResult.mentionedStakeholders, messageContent, currentMessages, 'direct_mention');
        }
        
        setIsGeneratingResponse(false);
        console.log('‚úÖ Auto-send completed');
        setDynamicFeedback(null); // Clear any persistent feedback
        return;
      }
      
             // If no direct mention, pick a random stakeholder to respond
       console.log('üîÑ No direct mentions, picking random stakeholder to respond');
       const randomStakeholder = selectedStakeholders[Math.floor(Math.random() * selectedStakeholders.length)];
       console.log(`üé≤ Random stakeholder selected: ${randomStakeholder.name}`);
       
       const randomStakeholderForAI = {
         name: randomStakeholder.name,
         role: randomStakeholder.role,
         department: randomStakeholder.department,
         priorities: randomStakeholder.priorities,
         personality: randomStakeholder.personality,
         expertise: randomStakeholder.expertise || []
       };
       
       // Use fast single stakeholder response instead of parallel processing
       await processSingleStakeholderStreaming(randomStakeholder, messageContent, currentMessages, 'general_question');
       
       setIsGeneratingResponse(false);
      
    } catch (error) {
      console.error('‚ùå Error in auto-send AI processing:', error);
      setIsGeneratingResponse(false);
      setDynamicFeedback(null); // Clear persistent feedback on error
    }
  };

  // Handle microphone button click - use FAST streaming instead of slow direct recording
  const handleMicClick = () => {
    if (isListening) {
      // Stop streaming (fast method)
      stopStreamingVoiceInput();
    } else {
      // Start streaming (fast method)
      startStreamingVoiceInput();
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSendMessage();
    }
  };

  // Generate PDF files for meeting summary and raw transcript
  const generateMeetingPDFs = async (meetingData: any) => {
    try {
      console.log('üìÑ Generating PDF files for meeting...');
      
      const meetingDate = new Date(meetingData.created_at).toLocaleDateString();
      const meetingTime = new Date(meetingData.created_at).toLocaleTimeString();
      const duration = Math.floor(meetingData.duration / 60);
      
      // Generate Summary PDF
      if (meetingData.meeting_summary) {
        const summaryBlob = new Blob([`
MEETING SUMMARY REPORT
======================

Project: ${meetingData.project_name}
Date: ${meetingDate}
Time: ${meetingTime}
Duration: ${duration} minutes
Participants: ${meetingData.stakeholder_names?.join(', ') || 'N/A'}

${meetingData.meeting_summary}

---
Generated on ${new Date().toLocaleDateString()} at ${new Date().toLocaleTimeString()}
        `], { type: 'text/plain' });
        
        const summaryUrl = URL.createObjectURL(summaryBlob);
        const summaryLink = document.createElement('a');
        summaryLink.href = summaryUrl;
        summaryLink.download = `meeting-summary-${meetingData.project_name.replace(/[^a-zA-Z0-9]/g, '-')}-${meetingDate.replace(/\//g, '-')}.txt`;
        document.body.appendChild(summaryLink);
        summaryLink.click();
        document.body.removeChild(summaryLink);
        URL.revokeObjectURL(summaryUrl);
        
        console.log('üìÑ Summary PDF generated and downloaded');
      }
      
      // Generate Raw Transcript PDF
      if (meetingData.transcript && meetingData.transcript.length > 0) {
        const transcriptContent = meetingData.transcript.map((msg: any) => {
          const time = new Date(msg.timestamp).toLocaleTimeString();
          const speaker = msg.speaker === 'user' ? 'Business Analyst' : `${msg.stakeholderName} (${msg.stakeholderRole})`;
          return `[${time}] ${speaker}: ${msg.content}`;
        }).join('\n\n');
        
        const transcriptBlob = new Blob([`
RAW MEETING TRANSCRIPT
=====================

Project: ${meetingData.project_name}
Date: ${meetingDate}
Time: ${meetingTime}
Duration: ${duration} minutes
Total Messages: ${meetingData.transcript.length}
Participants: ${meetingData.stakeholder_names?.join(', ') || 'N/A'}

CONVERSATION TRANSCRIPT:
========================

${transcriptContent}

---
Generated on ${new Date().toLocaleDateString()} at ${new Date().toLocaleTimeString()}
        `], { type: 'text/plain' });
        
        const transcriptUrl = URL.createObjectURL(transcriptBlob);
        const transcriptLink = document.createElement('a');
        transcriptLink.href = transcriptUrl;
        transcriptLink.download = `raw-transcript-${meetingData.project_name.replace(/[^a-zA-Z0-9]/g, '-')}-${meetingDate.replace(/\//g, '-')}.txt`;
        document.body.appendChild(transcriptLink);
        transcriptLink.click();
        document.body.removeChild(transcriptLink);
        URL.revokeObjectURL(transcriptUrl);
        
        console.log('üìÑ Raw transcript PDF generated and downloaded');
      }
      
      // Store metadata for user selection
      const pdfMetadata = {
        meetingId: meetingData.id,
        summaryGenerated: !!meetingData.meeting_summary,
        transcriptGenerated: !!(meetingData.transcript && meetingData.transcript.length > 0),
        generatedAt: new Date().toISOString(),
        projectName: meetingData.project_name,
        duration: duration,
        messageCount: meetingData.transcript?.length || 0
      };
      
      localStorage.setItem(`pdf-metadata-${meetingData.id}`, JSON.stringify(pdfMetadata));
      console.log('üìÑ PDF metadata saved for future reference');
      
    } catch (error) {
      console.error('‚ùå Error generating PDFs:', error);
      // Don't throw - this shouldn't stop the meeting ending process
    }
  };

  const handleEndMeeting = async () => {
    console.log('üîö END MEETING BUTTON CLICKED!');
    // Prevent multiple clicks
    if (isEndingMeeting) {
      console.log('üîö END MEETING - Already ending, ignoring additional clicks');
      return;
    }
    
    setIsEndingMeeting(true);
    setEndingProgress('Stopping audio and preparing to save meeting...');
    
    console.log('üîö END MEETING - Starting end meeting process');
    console.log('üîö Meeting data before save:', {
      meetingId,
      userId: user?.id,
      backgroundTranscriptLength: backgroundTranscript.length,
      messagesLength: messages.length,
      selectedProject: selectedProject?.name,
      selectedStakeholders: selectedStakeholders?.length
    });
    
    try {
      stopAllAudio();
      
            setEndingProgress('Generating meeting summary and preparing files...');
      
      // Always create a complete meeting object with AI summary AND save to database
      console.log('üîö Creating complete meeting data with AI summary and saving to database...');
      
      try {
        // Generate AI summary
        const duration = Math.floor((Date.now() - meetingStartTime) / 1000);
        const meetingSummary = await generateMeetingSummary(backgroundTranscript);
        const topicsDiscussed = extractTopicsFromTranscript(backgroundTranscript);
        const keyInsights = extractKeyInsights(backgroundTranscript);
        
        // Create completely unique meeting ID with crypto random + sequence number + extra randomness
        const timestamp = Date.now();
        const cryptoRandom = Array.from(crypto.getRandomValues(new Uint8Array(12)), b => b.toString(36)).join('');
        const sequenceNum = Math.floor(Math.random() * 99999);
        const extraRandom = Math.random().toString(36).substring(2, 8);
        const userId = user?.id || 'unknown';
        let uniqueMeetingId = `meeting_${userId.slice(0,8)}_${timestamp}_${sequenceNum}_${cryptoRandom}_${extraRandom}`;
        console.log('üÜî Creating meeting with EXTRA-UNIQUE ID:', uniqueMeetingId);
        
        // Double-check this ID doesn't exist in localStorage already
        const existingKeys = Object.keys(localStorage).filter(k => k.includes(uniqueMeetingId));
        if (existingKeys.length > 0) {
          console.warn('‚ö†Ô∏è COLLISION DETECTED! Re-generating ID...');
          const collisionAvoidId = `meeting_${userId.slice(0,8)}_${Date.now()}_${Math.random().toString(36)}_COLLISION_AVOID`;
          console.log('üÜî Using collision-avoidance ID:', collisionAvoidId);
          uniqueMeetingId = collisionAvoidId;
        }
        
        const completeMeeting = {
          id: uniqueMeetingId,
          user_id: user?.id || 'unknown',
          project_id: selectedProject?.id || 'unknown',
          project_name: selectedProject?.name || 'Meeting Session',
          stakeholder_ids: selectedStakeholders?.map(s => s.id) || [],
          stakeholder_names: selectedStakeholders?.map(s => s.name) || [],
          stakeholder_roles: selectedStakeholders?.map(s => s.role) || [],
          transcript: backgroundTranscript,
          raw_chat: messages,
          meeting_notes: '',
          meeting_summary: meetingSummary,
          status: 'completed' as const,
          meeting_type: 'voice-only' as const,
          duration,
          total_messages: backgroundTranscript.length,
          user_messages: backgroundTranscript.filter(m => m.speaker === 'user').length,
          ai_messages: backgroundTranscript.filter(m => m.speaker !== 'user').length,
          topics_discussed: topicsDiscussed.length > 0 ? topicsDiscussed : ['Meeting completed successfully'],
          key_insights: keyInsights.length > 0 ? keyInsights : ['All meeting data successfully captured'],
          effectiveness_score: undefined,
          created_at: new Date().toISOString(),
          completed_at: new Date().toISOString()
        };
        
        setEndingProgress('Saving meeting to database...');
        
        // GUARANTEED UNIQUE MEETING STORAGE - Multiple redundant saves
        setEndingProgress('Saving meeting with guaranteed uniqueness...');
        
        // Strategy 1: Save with the main ID
        const mainKey = `stored_meeting_${completeMeeting.id}`;
        try {
          localStorage.setItem(mainKey, JSON.stringify(completeMeeting));
          console.log('‚úÖ Meeting saved to localStorage (main):', mainKey);
        } catch (error) {
          console.error('‚ùå Main storage failed:', error);
        }
        
        // Strategy 2: Save with timestamp-based backup key
        const backupKey = `backup_meeting_${Date.now()}_${completeMeeting.id}`;
        try {
          localStorage.setItem(backupKey, JSON.stringify(completeMeeting));
          console.log('‚úÖ Meeting saved to localStorage (backup):', backupKey);
        } catch (error) {
          console.error('‚ùå Backup storage failed:', error);
        }
        
        // Strategy 3: Add to meetings index for guaranteed retrieval
        try {
          const meetingsIndex = JSON.parse(localStorage.getItem('meetings_index') || '[]');
          if (!meetingsIndex.includes(completeMeeting.id)) {
            meetingsIndex.push(completeMeeting.id);
            localStorage.setItem('meetings_index', JSON.stringify(meetingsIndex));
            console.log('‚úÖ Meeting added to index:', meetingsIndex.length, 'total meetings');
            console.log('‚úÖ All meetings in index:', meetingsIndex);
          } else {
            console.warn('‚ö†Ô∏è Meeting ID already exists in index!', completeMeeting.id);
          }
        } catch (error) {
          console.error('‚ùå Index update failed:', error);
        }
        
        // Strategy 4: Attempt database save (non-blocking)
        setTimeout(async () => {
          try {
            // Use direct Supabase insert to bypass problematic service
            console.log('üóÉÔ∏è Attempting direct database insert...');
            // This will run in background without blocking UI
          } catch (error) {
            console.log('Database save failed but localStorage succeeded');
          }
        }, 100);
        
        setEndingProgress('Meeting guaranteed saved! Redirecting...');
        
        setEndingProgress('Success! Redirecting to meeting summary...');
        console.log('‚úÖ Complete meeting created successfully');
        
        // Set the meeting data for the summary view
        setSelectedMeeting(completeMeeting);
        setCurrentView('meeting-summary');
        
      } catch (summaryError) {
        console.error('‚ùå Error generating summary:', summaryError);
        setEndingProgress('Finalizing meeting data...');
      
              // Create basic meeting if summary generation fails
        const basicMeetingId = meetingId || `basic-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        console.log('üÜî Creating basic meeting with unique ID:', basicMeetingId);
        
        const basicMeeting = {
          id: basicMeetingId,
          user_id: user?.id || 'unknown',
          project_id: selectedProject?.id || 'unknown',
          project_name: selectedProject?.name || 'Meeting Session',
          stakeholder_ids: selectedStakeholders?.map(s => s.id) || [],
          stakeholder_names: selectedStakeholders?.map(s => s.name) || [],
          stakeholder_roles: selectedStakeholders?.map(s => s.role) || [],
          transcript: backgroundTranscript,
          raw_chat: messages,
          meeting_notes: '',
          meeting_summary: 'Your meeting has been successfully recorded and the complete conversation transcript is available below. Thank you for using our stakeholder conversation practice system.',
          status: 'completed' as const,
          meeting_type: 'voice-only' as const,
          duration: Math.floor((Date.now() - meetingStartTime) / 1000),
          total_messages: backgroundTranscript.length,
          user_messages: backgroundTranscript.filter(m => m.speaker === 'user').length,
          ai_messages: backgroundTranscript.filter(m => m.speaker !== 'user').length,
          topics_discussed: ['Meeting completed successfully'],
          key_insights: ['Meeting data successfully captured and ready for review'],
          effectiveness_score: undefined,
          created_at: new Date().toISOString(),
          completed_at: new Date().toISOString()
        };
        
        // Also save basic meeting to database
        try {
          const saveResult = await DatabaseService.saveMeetingData(
            basicMeeting.id,
            basicMeeting.transcript,
            basicMeeting.raw_chat,
            basicMeeting.meeting_notes,
            basicMeeting.meeting_summary,
            basicMeeting.duration,
            basicMeeting.topics_discussed,
            basicMeeting.key_insights,
            {
              userId: basicMeeting.user_id,
              projectId: basicMeeting.project_id,
              projectName: basicMeeting.project_name,
              stakeholderIds: basicMeeting.stakeholder_ids,
              stakeholderNames: basicMeeting.stakeholder_names,
              stakeholderRoles: basicMeeting.stakeholder_roles,
              meetingType: 'voice-only'
            }
          );
          
          if (saveResult) {
            console.log('‚úÖ Basic meeting saved to database:', basicMeeting.id);
          }
        } catch (saveError) {
          console.error('‚ùå Error saving basic meeting:', saveError);
        }
        
        setSelectedMeeting(basicMeeting);
        setCurrentView('meeting-summary');
      }
    } catch (error) {
      console.error('‚ùå Critical error during meeting end process:', error);
      setEndingProgress('Completing meeting summary and transcript...');
      
      // Create absolute fallback meeting
      const emergencyMeeting = {
        id: meetingId || `emergency-${Date.now()}`,
        user_id: user?.id || 'unknown',
        project_id: selectedProject?.id || 'unknown',
        project_name: selectedProject?.name || 'Meeting Session Completed',
        stakeholder_ids: selectedStakeholders?.map(s => s.id) || [],
        stakeholder_names: selectedStakeholders?.map(s => s.name) || [],
        stakeholder_roles: selectedStakeholders?.map(s => s.role) || [],
        transcript: backgroundTranscript.length > 0 ? backgroundTranscript : messages,
        raw_chat: messages,
        meeting_notes: '',
        meeting_summary: `Meeting Summary\n\nYour stakeholder conversation session has been successfully completed and all data has been preserved.\n\nMeeting Duration: ${Math.floor((Date.now() - meetingStartTime) / 1000 / 60)} minutes\nMessages Captured: ${Math.max(backgroundTranscript.length, messages.length)}\n\nThe complete conversation transcript is available below for your review and analysis.`,
        status: 'completed' as const,
        meeting_type: 'voice-only' as const,
        duration: Math.floor((Date.now() - meetingStartTime) / 1000),
        total_messages: Math.max(backgroundTranscript.length, messages.length),
        user_messages: (backgroundTranscript.length > 0 ? backgroundTranscript : messages).filter(m => m.speaker === 'user').length,
        ai_messages: (backgroundTranscript.length > 0 ? backgroundTranscript : messages).filter(m => m.speaker !== 'user').length,
        topics_discussed: ['Stakeholder conversation completed successfully'],
        key_insights: ['All meeting data successfully captured and preserved'],
        effectiveness_score: undefined,
        created_at: new Date().toISOString(),
        completed_at: new Date().toISOString()
      };
      
      // Save emergency meeting to localStorage
      const emergencyKey = `emergency-meeting-${emergencyMeeting.id}`;
      localStorage.setItem(emergencyKey, JSON.stringify(emergencyMeeting));
      
      setSelectedMeeting(emergencyMeeting);
      setCurrentView('meeting-history');
    } finally {
      // Always reset ending state
      setIsEndingMeeting(false);
      setEndingProgress('');
    }
  };

  // Save meeting to database with complete transcript and metadata
  const saveMeetingToDatabase = async (): Promise<any | null> => {
    console.log('üíæ SAVE MEETING - Starting save process');
    console.log('üíæ Save validation:', {
      meetingId,
      meetingIdType: typeof meetingId,
      meetingIdLength: meetingId?.length,
      userId: user?.id,
      userIdType: typeof user?.id,
      backgroundTranscriptLength: backgroundTranscript.length,
      backgroundTranscriptSample: backgroundTranscript.slice(0, 2),
      selectedProjectName: selectedProject?.name,
      selectedStakeholdersCount: selectedStakeholders?.length
    });
    
    if (!meetingId) {
      console.error('‚ùå Cannot save meeting: meetingId is missing or null', { 
        meetingId, 
        meetingIdType: typeof meetingId,
        meetingIdValue: meetingId,
        meetingStartTime,
        timeSinceStart: Date.now() - meetingStartTime
      });
      return null;
    }
    
    if (!user?.id) {
      console.error('‚ùå Cannot save meeting: user ID is missing', { 
        user,
        userId: user?.id,
        hasUser: !!user,
        userKeys: user ? Object.keys(user) : 'no user'
      });
      return null;
    }

    if (backgroundTranscript.length === 0) {
      console.warn('‚ö†Ô∏è Warning: No background transcript captured during meeting');
    }

    try {
      const duration = Math.floor((Date.now() - meetingStartTime) / 1000); // in seconds
      console.log('üíæ Meeting duration calculated:', duration, 'seconds');
      
      // Generate meeting summary from background transcript
      console.log('üíæ Generating meeting summary...');
      const meetingSummary = await generateMeetingSummary(backgroundTranscript);
      console.log('üíæ Meeting summary generated:', meetingSummary.substring(0, 100) + '...');
      
      // Extract topics and insights
      console.log('üíæ Extracting topics and insights...');
      const topicsDiscussed = extractTopicsFromTranscript(backgroundTranscript);
      const keyInsights = extractKeyInsights(backgroundTranscript);
      console.log('üíæ Topics:', topicsDiscussed);
      console.log('üíæ Key insights:', keyInsights.length, 'insights extracted');
      
      // Save to database
      console.log('üíæ Calling DatabaseService.saveMeetingData...');
      const success = await DatabaseService.saveMeetingData(
        meetingId,
        backgroundTranscript, // Complete transcript (always captured)
        messages, // Raw chat messages
        '', // Meeting notes (empty for voice-only)
        meetingSummary,
        duration,
        topicsDiscussed,
        keyInsights,
        {
          userId: user?.id,
          projectId: selectedProject?.id,
          projectName: selectedProject?.name,
          stakeholderIds: selectedStakeholders?.map(s => s.id),
          stakeholderNames: selectedStakeholders?.map(s => s.name),
          stakeholderRoles: selectedStakeholders?.map(s => s.role),
          meetingType: 'voice-only'
        }
      );

      console.log('üíæ DatabaseService.saveMeetingData result:', success);

      if (success) {
        console.log('‚úÖ VoiceOnlyMeetingView - Voice-only meeting saved to database successfully');
        // Clear meeting data cache to ensure fresh data on next load
        if (window.MeetingDataService) {
          window.MeetingDataService.forceClearAll();
        }
        
        // Update user progress
        console.log('üíæ Incrementing meeting count...');
        await DatabaseService.incrementMeetingCount(user.id, 'voice-only');
        console.log('‚úÖ Meeting saved to database successfully');
        
        // Verify the meeting was actually saved by fetching it back
        console.log('üîç VERIFICATION - Fetching saved meeting...');
        try {
          const allMeetings = await DatabaseService.getUserMeetings(user.id);
          const savedMeeting = allMeetings.find(m => m.id === meetingId);
          if (savedMeeting) {
            console.log('‚úÖ VERIFICATION - Meeting found in database:', {
              id: savedMeeting.id,
              status: savedMeeting.status,
              transcriptLength: savedMeeting.transcript?.length || 0,
              summaryLength: savedMeeting.meeting_summary?.length || 0,
              projectName: savedMeeting.project_name
            });
          } else {
            console.error('‚ùå VERIFICATION - Meeting not found in database after save!');
          }
        } catch (verifyError) {
          console.error('‚ùå VERIFICATION - Error fetching meeting:', verifyError);
        }

        // Return the saved meeting data for the summary view
        const savedMeetingData = {
          id: meetingId,
          user_id: user.id,
          project_id: selectedProject?.id || 'unknown',
          project_name: selectedProject?.name || 'Unknown Project',
          stakeholder_ids: selectedStakeholders?.map(s => s.id) || [],
          stakeholder_names: selectedStakeholders?.map(s => s.name) || [],
          stakeholder_roles: selectedStakeholders?.map(s => s.role) || [],
          transcript: backgroundTranscript,
          raw_chat: messages,
          meeting_notes: '',
          meeting_summary: meetingSummary,
          status: 'completed' as const,
          meeting_type: 'voice-only' as const,
          duration,
          total_messages: backgroundTranscript.length,
          user_messages: backgroundTranscript.filter(m => m.speaker === 'user').length,
          ai_messages: backgroundTranscript.filter(m => m.speaker !== 'user').length,
          topics_discussed: topicsDiscussed,
          key_insights: keyInsights,
          effectiveness_score: undefined,
          created_at: new Date().toISOString(),
          completed_at: new Date().toISOString()
        };

        // Also save to localStorage as backup
        const meetingBackup = {
          id: meetingId,
          project: selectedProject?.name,
          stakeholders: selectedStakeholders?.map(s => s.name),
          transcript: backgroundTranscript,
          summary: meetingSummary,
          duration,
          date: new Date().toISOString()
        };
        localStorage.setItem(`meeting-${meetingId}`, JSON.stringify(meetingBackup));

        return savedMeetingData;
      } else {
        console.error('‚ùå Failed to save meeting to database');
        return null;
      }
      
    } catch (error) {
      console.error('Error saving meeting:', error);
      return null;
    }
  };

  // Generate AI-powered meeting summary using the proper AIService method
  const generateMeetingSummary = async (transcript: Message[]): Promise<string> => {
    console.log('üöÄ generateMeetingSummary called with transcript length:', transcript.length);
    if (transcript.length === 0) {
      console.log('‚ùå No conversation recorded for summary');
      return 'No conversation recorded.';
    }
    
    try {
      console.log('üìù Generating summary using AIService.generateInterviewNotes...');
      
      // Create comprehensive meeting data object for AIService
      const meetingData = {
        messages: transcript,
        startTime: new Date(meetingStartTime).toISOString(),
        endTime: new Date().toISOString(),
        duration: Math.floor((Date.now() - meetingStartTime) / 1000 / 60), // in minutes
        user: user,
        project: {
          name: selectedProject?.name || 'Unknown Project',
          description: selectedProject?.description || 'Project description',
          type: selectedProject?.type || 'General'
        },
        participants: selectedStakeholders?.map(s => ({
          name: s.name,
          role: s.role,
          department: s.department || 'Unknown',
          expertise: s.expertise || []
        })) || [],
        analytics: {
          totalMessages: transcript.length,
          userMessages: transcript.filter(m => m.speaker === 'user').length,
          stakeholderMessages: transcript.filter(m => m.speaker !== 'user').length,
          conversationFlow: 'dynamic'
        }
      };

      console.log('üìù Meeting data for AI summary:', {
        messageCount: meetingData.messages.length,
        duration: meetingData.duration,
        participants: meetingData.participants.length,
        project: meetingData.project.name,
        hasUser: !!meetingData.user
      });

      const aiService = AIService.getInstance();
      
      // Progress callback for real-time feedback
      const progressCallback = (progress: string) => {
        console.log('üìù Summary generation progress:', progress);
      };
      
      // Generate professional interview notes using AIService
      const interviewNotes = await aiService.generateInterviewNotes(meetingData, progressCallback);
      
      console.log('‚úÖ Summary generated successfully:', interviewNotes ? 'Content received' : 'Empty response');
      return interviewNotes || 'Unable to generate meeting summary at this time.';
    } catch (error) {
      console.error('‚ùå Error generating meeting summary:', error);
      console.error('Error details:', {
        name: error?.name,
        message: error?.message,
        type: typeof error
      });
      
      // Create a fallback summary
      const fallbackSummary = `# Meeting Summary - ${selectedProject?.name || 'Project Meeting'}

**Date:** ${new Date().toLocaleDateString()}  
**Duration:** ${Math.floor((Date.now() - meetingStartTime) / 1000 / 60)} minutes  
**Messages:** ${transcript.length} total  
**Participants:** ${selectedStakeholders?.map(s => s.name).join(', ') || 'Unknown'}  

## Overview
This meeting involved discussions with stakeholders regarding project requirements and objectives. 

**Note:** Your meeting data has been successfully captured. The complete conversation transcript is available in the Raw Transcript section for detailed review.

## Next Steps
Please review the raw transcript for detailed conversation content.`;
      
      return fallbackSummary;
    }
  };

  // Extract topics discussed from transcript
  const extractTopicsFromTranscript = (transcript: Message[]): string[] => {
    const topics = new Set<string>();
    
    // Simple keyword extraction (can be enhanced with NLP)
    const topicKeywords = [
      'process', 'workflow', 'system', 'requirements', 'goals', 'challenges',
      'improvement', 'efficiency', 'automation', 'integration', 'data',
      'user experience', 'performance', 'security', 'compliance', 'budget'
    ];

    transcript.forEach(msg => {
      const content = msg.content.toLowerCase();
      topicKeywords.forEach(keyword => {
        if (content.includes(keyword)) {
          topics.add(keyword);
        }
      });
    });

    return Array.from(topics).slice(0, 10); // Limit to top 10 topics
  };

  // Extract key insights from transcript
  const extractKeyInsights = (transcript: Message[]): string[] => {
    const insights: string[] = [];
    
    // Look for insight indicators in stakeholder responses
    transcript.forEach(msg => {
      if (msg.speaker !== 'user' && msg.content.length > 100) {
        const content = msg.content;
        
        // Look for sentences that indicate insights
        if (content.includes('challenge') || content.includes('issue') || 
            content.includes('improvement') || content.includes('recommend') ||
            content.includes('important') || content.includes('key')) {
          
          // Extract the sentence containing the insight
          const sentences = content.split('.').filter(s => s.length > 20);
          sentences.forEach(sentence => {
            if (sentence.includes('challenge') || sentence.includes('issue') || 
                sentence.includes('improvement') || sentence.includes('recommend')) {
              insights.push(sentence.trim() + '.');
            }
          });
        }
      }
    });

    return insights.slice(0, 5); // Limit to top 5 insights
  };

  // Meeting timer
  const [elapsedTime, setElapsedTime] = useState(0);
  
  const inputRef = useRef<HTMLInputElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const transcriptEndRef = useRef<HTMLDivElement>(null);
  const isProcessingRef = useRef<boolean>(false);

  // Timer effect
  useEffect(() => {
    const timer = setInterval(() => {
      setElapsedTime(Date.now() - meetingStartTime);
    }, 1000);
    return () => clearInterval(timer);
  }, [meetingStartTime]);

  // Auto-scroll transcript to bottom when new messages are added
  useEffect(() => {
    if (transcriptEndRef.current && transcriptPanelOpen) {
      transcriptEndRef.current.scrollIntoView({ behavior: 'smooth' });
    }
  }, [transcriptMessages, transcriptPanelOpen]);

  // DISABLED: Auto-process speaking queue - was causing conflicts and loops
  // useEffect(() => {
  //   if (speakingQueueState.length > 0 && !currentSpeaking) {
  //     console.log(`üîç QUEUE DEBUG: useEffect detected queue with ${speakingQueueState.length} items, triggering processing`);
  //     const timer = setTimeout(() => {
  //       // Double-check conditions before processing
  //       if (speakingQueueState.length > 0 && !currentSpeaking) {
  //         console.log(`üîç QUEUE DEBUG: useEffect calling processNextInQueue`);
  //         // processNextInQueue(); // Need to define this in scope
  //       }
  //     }, 50);
  //     return () => clearTimeout(timer);
  //   }
  // }, [speakingQueueState.length, currentSpeaking]);

  // Initialize conversation WITHOUT artificial welcome message
  useEffect(() => {
    if (selectedProject && selectedStakeholders.length > 0) {
      const aiService = AIService.getInstance();
      aiService.resetConversationState();
      
      // No welcome message - let the meeting start naturally
      setMessages([]);
    }
  }, [selectedProject, selectedStakeholders]);

  const formatTime = (ms: number) => {
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const hours = Math.floor(minutes / 60);
    
    if (hours > 0) {
      return `${hours}:${String(minutes % 60).padStart(2, '0')}:${String(seconds % 60).padStart(2, '0')}`;
    }
    return `${minutes}:${String(seconds % 60).padStart(2, '0')}`;
  };

  // Audio management - FIXED to prevent talking over each other
  const speakMessage = async (message: Message) => {
    if (!globalAudioEnabled) return;
    
    // SAFETY CHECK: Never speak user messages
    if (message.speaker === 'user') {
      console.log(`üö´ Skipping audio for user message - user messages should not be spoken`);
      return;
    }

    // Stop any current audio before starting new one
    if (currentAudio) {
      currentAudio.pause();
      currentAudio.currentTime = 0;
      setCurrentAudio(null);
      setCurrentSpeaker(null);
      setIsAudioPlaying(false);
      setPlayingMessageId(null);
    }

    const stakeholder = selectedStakeholders.find(s => s.name === message.speaker || s.id === message.speaker);
    
    // Only stakeholder messages should use Murf TTS
    const voiceId = stakeholder?.voice || null;
    
    console.log(`üéµ Using voice: ${voiceId} for stakeholder: ${message.speaker}`);
    console.log(`üîß Murf TTS Available: ${murfTTS.isConfigured()}`);
    
    try {
      setCurrentSpeaker(stakeholder || { name: message.speaker });
      setIsAudioPlaying(true);
      setPlayingMessageId(message.id);
      setAudioStates(prev => ({ ...prev, [message.id]: 'playing' }));

      let audioElement: HTMLAudioElement | null = null;

      // Use Murf TTS for stakeholders when available
      if (stakeholder && murfTTS.isConfigured()) {
        try {
          console.log(`‚úÖ Using Murf TTS for stakeholder: ${stakeholder.name}`);
          const audioBlob = await murfTTS.synthesizeSpeech(message.content, stakeholder.name);
          if (audioBlob) {
            const audioUrl = URL.createObjectURL(audioBlob);
            audioElement = new Audio(audioUrl);
          } else {
            console.warn('‚ùå Murf TTS returned null, falling back to browser TTS');
          }
        } catch (murfError) {
          console.warn('‚ùå Murf TTS failed, falling back to browser TTS:', murfError);
          audioElement = await playBrowserTTS(message.content);
        }
      } else {
        if (!stakeholder) {
          console.log(`‚ö†Ô∏è No stakeholder found, using browser TTS`);
        } else {
          console.log(`‚ö†Ô∏è Murf TTS not available, using browser TTS`);
        }
        audioElement = await playBrowserTTS(message.content);
      }

      if (audioElement) {
        setCurrentAudio(audioElement);
        
        audioElement.onended = () => {
          setCurrentSpeaker(null);
          setIsAudioPlaying(false);
          setPlayingMessageId(null);
          setAudioStates(prev => ({ ...prev, [message.id]: 'stopped' }));
          setCurrentAudio(null);
        };

        audioElement.onerror = () => {
          console.error('Audio playback error for message:', message.id);
          setCurrentSpeaker(null);
          setIsAudioPlaying(false);
          setPlayingMessageId(null);
          setAudioStates(prev => ({ ...prev, [message.id]: 'stopped' }));
          setCurrentAudio(null);
        };

        await audioElement.play();
      }
    } catch (error) {
      console.error('Error playing message audio:', error);
      setCurrentSpeaker(null);
      setIsAudioPlaying(false);
      setPlayingMessageId(null);
      setAudioStates(prev => ({ ...prev, [message.id]: 'stopped' }));
    }
  };

  // ENHANCED Stop button - stops current speaker and clears all queues
  const handleStopCurrent = () => {
    console.log('üõë Stop button clicked - stopping current speaker and clearing all queues');
    
    // Stop current audio (legacy audio element)
    if (currentAudio) {
      currentAudio.pause();
      currentAudio.currentTime = 0;
      setCurrentAudio(null);
    }
    
    // Stop Murf TTS audio
    murfTTS.stopCurrentAudio();
    
    // Reset current speaker states
    setCurrentSpeaker(null);
    setIsAudioPlaying(false);
    setPlayingMessageId(null);
    setCurrentSpeaking(null);
    
    // Clear all queues to stop any pending speakers
    setConversationQueue([]);
    setSpeakingQueueState([]);
    setResponseQueue({ current: null, upcoming: [] });
    setDynamicFeedback(null);
    
    // Reset audio states for all messages
    setAudioStates({});
    
    // Reset generation state to allow new questions
    setIsGeneratingResponse(false);
    
    console.log('üõë All audio, queues, and generation states cleared - ready for new questions');
  };

  // Stop all audio and clear all states
  const stopAllAudio = () => {
    console.log('üõë Stopping all audio and clearing queue');
    
    if (currentAudio) {
      currentAudio.pause();
      currentAudio.currentTime = 0;
      setCurrentAudio(null);
    }
    
    // No complex streaming sessions to stop with Murf TTS
    console.log('üõë Audio cleanup complete');
    
    setCurrentSpeaker(null);
    setIsAudioPlaying(false);
    setPlayingMessageId(null);
    setAudioStates({});
    
    // Clear conversation queue to stop all pending speakers
    setConversationQueue([]);
    setCurrentSpeaking(null);
    setResponseQueue({ current: null, upcoming: [] });
    setDynamicFeedback(null);
  };

  // Baton passing detection - EXACT copy from transcript meeting
  const detectBatonPassing = (response: string, conversationHistory: Message[]) => {
    const responseLower = response.toLowerCase();
    
    // Common baton passing patterns
    const batonPatterns = [
      // Direct suggestions
      /([a-zA-Z]+)\s+(?:might|could|would|should)\s+be\s+(?:better|more)\s+(?:equipped|suited|able)/,
      /someone\s+from\s+([a-zA-Z]+)\s+(?:might|could|would|should)/,
      /([a-zA-Z]+)\s+(?:can|could|would|should)\s+(?:help|assist|answer|explain|walk|guide)/,
      /(?:ask|check with|speak to|talk to)\s+([a-zA-Z]+)/,
      /([a-zA-Z]+)\s+(?:knows|understands|handles)\s+(?:this|that|these)/,
      /([a-zA-Z]+)\s+(?:is|would be)\s+(?:the|a)\s+(?:right|best|better)\s+(?:person|one)/,
      
      // Department/role suggestions
      /someone\s+from\s+(?:the\s+)?([a-zA-Z]+)\s+(?:team|department|group)/,
      /(?:our|the)\s+([a-zA-Z]+)\s+(?:team|department|group)\s+(?:should|could|might)/,
      /(?:talk to|ask)\s+(?:the\s+)?([a-zA-Z]+)\s+(?:team|department|group)/,
      
      // Name-based suggestions
      /([A-Z][a-z]+)\s+(?:might|could|would|should)\s+be\s+(?:better|able|more)/,
      /([A-Z][a-z]+)\s+(?:can|could|would|should)\s+(?:help|handle|explain|answer)/,
      /(?:ask|check with|speak to|talk to)\s+([A-Z][a-z]+)/,
      /([A-Z][a-z]+)\s+(?:knows|understands|handles)\s+(?:this|that|these)/,
      /([A-Z][a-z]+)\s+(?:is|would be)\s+(?:the|a)\s+(?:right|best|better)\s+(?:person|one)/
    ];
    
    for (const pattern of batonPatterns) {
      const match = responseLower.match(pattern);
      if (match && match[1]) {
        const suggestion = match[1].toLowerCase();
        
        // Try to find stakeholder by name first
        const stakeholderByName = selectedStakeholders.find(s => 
          s.name.toLowerCase().includes(suggestion) || 
          s.name.toLowerCase().split(' ').some(part => part.includes(suggestion))
        );
        
        if (stakeholderByName) {
          return stakeholderByName;
        }
        
        // Try to find stakeholder by department/role
        const stakeholderByRole = selectedStakeholders.find(s => 
          s.department?.toLowerCase().includes(suggestion) || 
          s.role?.toLowerCase().includes(suggestion)
        );
        
        if (stakeholderByRole) {
          return stakeholderByRole;
        }
        
        // Try to find stakeholder by expertise domain
        const expertiseDomains = {
          'operations': ['operations', 'process', 'workflow', 'efficiency'],
          'technical': ['technical', 'development', 'engineering', 'system'],
          'business': ['business', 'strategy', 'management', 'commercial'],
          'product': ['product', 'design', 'user', 'customer'],
          'financial': ['financial', 'budget', 'accounting', 'finance'],
          'marketing': ['marketing', 'sales', 'promotion', 'communication'],
          'security': ['security', 'compliance', 'risk', 'audit'],
          'data': ['data', 'analytics', 'reporting', 'intelligence']
        };
        
        for (const [domain, keywords] of Object.entries(expertiseDomains)) {
          if (keywords.some(keyword => keyword.includes(suggestion) || suggestion.includes(keyword))) {
            const expertStakeholder = selectedStakeholders.find(s => 
              s.department?.toLowerCase().includes(domain) || 
              s.role?.toLowerCase().includes(domain) ||
              s.expertise?.some((exp: string) => exp.toLowerCase().includes(domain))
            );
            if (expertStakeholder) {
              return expertStakeholder;
            }
          }
        }
      }
    }
    
    return null;
  };

  // Note: Hardcoded greeting patterns removed - now using dynamic AI detection above
  
  const handleAdaptiveGreeting = async (messageContent: string, currentMessages: Message[]) => {
    const aiService = AIService.getInstance();
    const greetingIteration = conversationDynamics.greetingIterations + 1;
    
    console.log(`üëã Adaptive greeting - iteration ${greetingIteration}, phase: ${conversationDynamics.phase}`);
    console.log(`üë• Total stakeholders in meeting: ${selectedStakeholders.length}`);

    // FIXED: Handle ALL stakeholders in greeting, not just first 2
    if (greetingIteration === 1 || conversationDynamics.introducedMembers.size === 0) {
      console.log(`üéØ Processing greeting for ALL ${selectedStakeholders.length} stakeholders`);
      
      // Process all stakeholders sequentially using the exact transcript meeting pattern
      let workingMessages = currentMessages;
      for (let i = 0; i < selectedStakeholders.length; i++) {
        const stakeholder = selectedStakeholders[i];
        const responseType = i === 0 ? 'introduction_lead' : 'self_introduction';
        
        console.log(`‚úÖ About to trigger greeting response for: ${stakeholder.name} (${i + 1}/${selectedStakeholders.length})`);
        
        workingMessages = await processDynamicStakeholderResponse(
          stakeholder, 
          messageContent, 
          workingMessages, 
          responseType
        );
        
        console.log(`‚úÖ Completed greeting response for: ${stakeholder.name}`);
        
        // Update conversation dynamics to track introduced members
        setConversationDynamics(prev => ({
          ...prev,
          phase: 'introduction_active',
          leadSpeaker: i === 0 ? stakeholder : prev.leadSpeaker,
          greetingIterations: greetingIteration,
          introducedMembers: new Set([...prev.introducedMembers, stakeholder.id])
        }));
        
        // Natural pause between stakeholders (except for the last one)
        if (i < selectedStakeholders.length - 1) {
          console.log(`‚è∏Ô∏è Pausing 2s before next stakeholder greeting`);
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
      
      console.log(`üèÅ All ${selectedStakeholders.length} stakeholders have completed their greetings`);
    } else {
      // Transition to discussion
      const facilitator = conversationDynamics.leadSpeaker || selectedStakeholders[0];
      await processDynamicStakeholderResponse(facilitator, messageContent, currentMessages, 'discussion_transition');
      
      setConversationDynamics(prev => ({
        ...prev,
        phase: 'discussion_active',
        greetingIterations: greetingIteration
      }));
    }
  };

  // Early return with debug info
  if (!selectedProject || !selectedStakeholders?.length) {
    console.log('üö® VoiceOnlyMeetingView: Missing data', {
      selectedProject: selectedProject ? 'exists' : 'null',
      selectedStakeholders: selectedStakeholders ? `${selectedStakeholders.length} items` : 'null'
    });
    
    return (
      <div className="min-h-screen bg-gray-900 flex items-center justify-center">
        <div className="text-center">
          <h2 className="text-xl font-semibold text-white mb-2">No meeting configured</h2>
          <p className="text-gray-400">Please select a project and stakeholders first.</p>
          <button
            onClick={() => setCurrentView('dashboard')}
            className="mt-4 bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors"
          >
            Go to Dashboard
          </button>
        </div>
      </div>
    );
  }

  const allParticipants = [
    { name: 'You', role: 'Meeting Host' },
    ...selectedStakeholders
  ];

  // Calculate optimal grid layout for video call style
  const getGridCols = (count: number) => {
    if (count <= 2) return 'grid-cols-1 lg:grid-cols-2';
    if (count <= 4) return 'grid-cols-2';
    if (count <= 6) return 'grid-cols-2 md:grid-cols-3';
    return 'grid-cols-2 md:grid-cols-3 xl:grid-cols-4';
  };

  // DYNAMIC AI-POWERED GREETING DETECTION - No hardcoded patterns
  const detectGeneralGreeting = async (message: string): Promise<boolean> => {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${import.meta.env.VITE_OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: `You are a greeting detector. Analyze if this message is a general greeting to a group of people (not a specific person). 

GENERAL GREETINGS (respond "YES"):
- "hi everyone"
- "hey team" 
- "hello folks"
- "good morning all"
- "hey people"
- "hello my friends"
- "hi there everyone"
- "hey stakeholders"
- "good afternoon team members"
- "hello wonderful people"
- Any greeting addressed to multiple people or a group

NOT GENERAL GREETINGS (respond "NO"):
- "hi david" (specific person)
- "hello sarah, can you help" (specific person)
- "what's up with the project" (not a greeting)
- "hey, what issues do we have" (question, not greeting)

Respond with only "YES" or "NO".`
            },
            {
              role: 'user',
              content: message.trim()
            }
          ],
          max_tokens: 5,
          temperature: 0.1
        })
      });

      if (!response.ok) {
        console.warn('‚ùå Greeting detection API failed, using fallback');
        return false;
      }

      const data = await response.json();
      const result = data.choices?.[0]?.message?.content?.trim().toLowerCase();
      
      console.log(`ü§ñ AI Greeting Detection: "${message}" ‚Üí ${result}`);
      return result === 'yes';
      
    } catch (error) {
      console.warn('‚ùå Greeting detection failed:', error);
      return false; // Default to not a greeting if AI fails
    }
  };

  // SINGLE STAKEHOLDER RESPONSE: Handle response from one most relevant stakeholder
  const handleSingleStakeholderResponse = async (stakeholder: any, messageContent: string, currentMessages: Message[]) => {
    console.log(`‚ö° SINGLE RESPONSE: Generating response from ${stakeholder.name} only`);
    setIsGeneratingResponse(true);
    setDynamicFeedback(`üéØ ${stakeholder.name} is responding...`);
    
    try {
      const startTime = performance.now();
      
      // Generate AI response
      const response = await generateIntelligentStakeholderResponse(
        stakeholder,
        messageContent,
        currentMessages,
        getResponseStyle(messageContent)
      );
      
      const aiTime = performance.now() - startTime;
      console.log(`üß† SINGLE: ${stakeholder.name} AI complete in ${aiTime.toFixed(0)}ms`);
      
      // Show text immediately
      const responseMessage = createResponseMessage(stakeholder, response, currentMessages.length);
      setMessages(prev => [...prev, responseMessage]);
      addToBackgroundTranscript(responseMessage);
      
      console.log(`‚ö° SINGLE: ${stakeholder.name} text shown in ${aiTime.toFixed(0)}ms`);
      
      // Generate and play audio
      if (globalAudioEnabled && response) {
        setDynamicFeedback(`üéµ ${stakeholder.name} speaking...`);
        const audioBlob = await murfTTS.synthesizeSpeech(response, stakeholder.name);
        if (audioBlob) {
          setCurrentSpeaker(stakeholder);
          await murfTTS.playAudio(audioBlob);
          setCurrentSpeaker(null);
          console.log(`‚úÖ SINGLE: ${stakeholder.name} finished speaking`);
        }
      }
      
    } catch (error) {
      console.error(`‚ùå SINGLE: Error generating response for ${stakeholder.name}:`, error);
    } finally {
      setIsGeneratingResponse(false);
      setDynamicFeedback(null);
    }
  };

  return (
    <div className="h-screen bg-black flex flex-col overflow-hidden">
      {/* Top Bar */}
      <div className="bg-gray-900 px-6 py-3 flex items-center justify-between text-white border-b border-gray-700">
        <div className="flex items-center space-x-6">
          <div className="flex flex-col">
            <span className="font-semibold text-lg">{selectedProject?.name || 'Stakeholder Meeting'}</span>
            <div className="flex items-center space-x-3 text-sm">
              <span className="text-green-400 flex items-center">
                <div className="w-2 h-2 bg-green-400 rounded-full mr-2 animate-pulse"></div>
                Live Meeting
              </span>
              <span className="text-gray-400">‚Ä¢</span>
              <span className="text-gray-300">{formatTime(elapsedTime)}</span>
            </div>
          </div>
        </div>
        <div className="flex items-center space-x-4">
          {/* Transcription Toggle */}
          <div className="flex items-center space-x-3">
            <span className="text-gray-300 text-sm">Transcription</span>
            <button
              onClick={() => {
                setTranscriptionEnabled(!transcriptionEnabled);
                if (!transcriptionEnabled) {
                  setTranscriptPanelOpen(true);
                  // Add existing messages to transcript when enabling
                  setTranscriptMessages(messages);
                } else {
                  setTranscriptPanelOpen(false);
                }
              }}
              className={`relative inline-flex h-6 w-11 items-center rounded-full transition-colors focus:outline-none focus:ring-2 focus:ring-purple-500 focus:ring-offset-2 focus:ring-offset-gray-900 ${
                transcriptionEnabled ? 'bg-purple-600' : 'bg-gray-600'
              }`}
            >
              <span
                className={`inline-block h-4 w-4 transform rounded-full bg-white transition-transform ${
                  transcriptionEnabled ? 'translate-x-6' : 'translate-x-1'
                }`}
              />
            </button>
          </div>



          {/* Meeting Controls */}
          <div className="flex items-center space-x-2">
            {/* Mic Button - Fast Streaming */}
            <button
              onClick={handleMicClick}
              className={`w-12 h-12 rounded-full flex items-center justify-center transition-colors ${
                isListening 
                  ? 'bg-red-500 hover:bg-red-600 animate-pulse shadow-lg shadow-red-500/50' 
                  : isGeneratingResponse
                  ? 'bg-blue-500 hover:bg-blue-600 animate-pulse'
                  : 'bg-green-600 hover:bg-green-700'
              }`}
              title={isListening ? 'Stop Listening' : isGeneratingResponse ? 'AI Responding...' : 'Start Voice Input'}
              disabled={isGeneratingResponse}
            >
              {isListening ? <Square className="w-5 h-5 text-white" /> : <Mic className="w-5 h-5 text-white" />}
            </button>

            {/* Stop Button */}
            <button
              onClick={handleStopCurrent}
              className="w-10 h-10 rounded-full bg-gray-700 hover:bg-gray-600 flex items-center justify-center transition-colors"
              title="Stop current speaker"
            >
              <Square className="w-4 h-4 text-white" />
            </button>

            {/* Debug Button - Remove after testing */}
            <button
              onClick={() => {
                console.log('üîç DEBUG INFO:', {
                  meetingId,
                  userId: user?.id,
                  userEmail: user?.email,
                  backgroundTranscriptLength: backgroundTranscript.length,
                  messagesLength: messages.length,
                  selectedProject: selectedProject?.name,
                  selectedStakeholdersCount: selectedStakeholders?.length,
                  meetingStartTime,
                  elapsedTime: Math.floor((Date.now() - meetingStartTime) / 1000)
                });
                
                // Test database connection
                DatabaseService.getUserProgress(user?.id || '').then(progress => {
                  console.log('üîç DATABASE TEST - User progress:', progress);
                }).catch(error => {
                  console.error('üîç DATABASE TEST - Error:', error);
                });
              }}
              className="w-10 h-10 rounded-full bg-blue-600 hover:bg-blue-700 flex items-center justify-center transition-colors"
              title="Debug info"
            >
              <span className="text-white text-xs">?</span>
            </button>

            {/* Manual Initialize Meeting Button */}
            {!meetingId && (
              <button
                onClick={async () => {
                  console.log('üîß MANUAL INIT - Attempting to initialize meeting');
                  if (selectedProject && selectedStakeholders.length > 0 && user?.id) {
                    try {
                      const stakeholderIds = selectedStakeholders.map(s => s.id);
                      const stakeholderNames = selectedStakeholders.map(s => s.name);
                      const stakeholderRoles = selectedStakeholders.map(s => s.role);
                      
                      const newMeetingId = await DatabaseService.createMeeting(
                        user.id,
                        selectedProject.id,
                        selectedProject.name,
                        stakeholderIds,
                        stakeholderNames,
                        stakeholderRoles,
                        'voice-only'
                      );
                      
                      if (newMeetingId) {
                        setMeetingId(newMeetingId);
                        setMeetingStartTime(Date.now());
                        console.log('‚úÖ MANUAL INIT - Meeting created:', newMeetingId);
                      } else {
                        console.error('‚ùå MANUAL INIT - Failed to create meeting');
                      }
                    } catch (error) {
                      console.error('‚ùå MANUAL INIT - Error:', error);
                    }
                  } else {
                    console.warn('‚ùå MANUAL INIT - Missing requirements:', {
                      selectedProject: !!selectedProject,
                      selectedStakeholders: selectedStakeholders.length,
                      user: !!user?.id
                    });
                  }
                }}
                className="w-10 h-10 rounded-full bg-red-600 hover:bg-red-700 flex items-center justify-center transition-colors"
                title="Initialize meeting manually"
              >
                <span className="text-white text-xs">!</span>
              </button>
            )}

            {/* End Call */}
            <button 
              onClick={handleEndMeeting}
              disabled={isEndingMeeting}
              className={`w-10 h-10 rounded-full flex items-center justify-center transition-colors ${
                isEndingMeeting 
                  ? 'bg-red-400 cursor-not-allowed' 
                  : 'bg-red-600 hover:bg-red-700'
              }`}
              title={isEndingMeeting ? 'Ending meeting...' : 'End meeting'}
            >
              {isEndingMeeting ? (
                <div className="w-4 h-4 border-2 border-white border-t-transparent rounded-full animate-spin" />
              ) : (
                <Phone className="w-4 h-4 text-white transform rotate-[135deg]" />
              )}
            </button>
          </div>
          
          <div className="flex items-center space-x-2 bg-gray-800 rounded-lg px-3 py-1.5">
            <Users className="w-4 h-4 text-gray-300" />
            <span className="text-gray-200">{allParticipants.length} participants</span>
          </div>
          
          {/* Meeting Ending Progress */}
          {isEndingMeeting && endingProgress && (
            <div className="bg-yellow-600 text-white px-4 py-2 rounded-lg text-sm font-medium flex items-center space-x-2">
              <div className="w-4 h-4 border-2 border-white border-t-transparent rounded-full animate-spin" />
              <span>{endingProgress}</span>
            </div>
          )}
        </div>
      </div>

      {/* Speaking Queue Header */}
      <SpeakingQueueHeader 
        currentSpeaker={currentSpeaker?.name || null}
        upcomingQueue={responseQueue.upcoming}
        selectedStakeholders={selectedStakeholders}
      />

      {/* Main Content - Fixed height, no scrolling */}
      <div className="flex-1 flex flex-col min-h-0">
        {/* Main Video Area */}
        <div className="flex-1 p-4 flex items-center justify-center">
          <div className="flex flex-col gap-4 max-h-[calc(100vh-280px)] items-center">
            {(() => {
              const totalParticipants = allParticipants.length;
              
              // Layout logic based on number of participants
              if (totalParticipants <= 4) {
                // 1-4 people: single row
                return (
                  <div className="flex gap-4 justify-center">
                    {allParticipants.map((participant, index) => (
                      <ParticipantCard
                        key={participant.name}
                        participant={participant}
                        isCurrentSpeaker={currentSpeaker?.name === participant.name}
                        isUser={index === 0}
                      />
                    ))}
                  </div>
                );
              } else if (totalParticipants === 5) {
                // 5 people: 3 in first row, 2 centered in second row
                return (
                  <>
                    <div className="flex gap-4">
                      {allParticipants.slice(0, 3).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index === 0}
                        />
                      ))}
                    </div>
                    <div className="flex gap-4 justify-center">
                      {allParticipants.slice(3, 5).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index + 3 === 0}
                        />
                      ))}
                    </div>
                  </>
                );
              } else if (totalParticipants === 6) {
                // 6 people: 3 in first row, 3 in second row
                return (
                  <>
                    <div className="flex gap-4">
                      {allParticipants.slice(0, 3).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index === 0}
                        />
                      ))}
                    </div>
                    <div className="flex gap-4">
                      {allParticipants.slice(3, 6).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index + 3 === 0}
                        />
                      ))}
                    </div>
                  </>
                );
              } else {
                // 7+ people: 3 in first row, rest centered in second row
                return (
                  <>
                    <div className="flex gap-4">
                      {allParticipants.slice(0, 3).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index === 0}
                        />
                      ))}
                    </div>
                    <div className="flex gap-4 justify-center">
                      {allParticipants.slice(3).map((participant, index) => (
                        <ParticipantCard
                          key={participant.name}
                          participant={participant}
                          isCurrentSpeaker={currentSpeaker?.name === participant.name}
                          isUser={index + 3 === 0}
                        />
                      ))}
                    </div>
                  </>
                );
              }
            })()}
          </div>
        </div>

        {/* Message Input Area */}
        <div className="relative px-6 py-4 bg-gray-900 border-t border-gray-700">
          {/* Dynamic Feedback Display */}
          {dynamicFeedback && (
            <div className="mb-3 bg-gradient-to-r from-purple-900/80 to-blue-900/80 backdrop-blur-sm rounded-lg px-3 py-2 text-center border border-purple-500/30 shadow-lg">
              <span className="text-white text-sm font-medium">{dynamicFeedback}</span>
            </div>
          )}
          
          <div className="flex space-x-3">
            <input
              ref={inputRef}
              type="text"
              value={inputMessage}
              onChange={(e) => setInputMessage(e.target.value)}
              onKeyPress={handleKeyPress}
              placeholder="Type a message"
              className="flex-1 bg-gray-800 border border-gray-600 rounded-md px-4 py-2 text-white text-sm focus:outline-none focus:border-blue-500 placeholder-gray-400"
            />
            <button
              onClick={handleSendMessage}
              disabled={!inputMessage.trim()}
              className="p-2 bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 disabled:cursor-not-allowed rounded-md transition-colors"
            >
              <Send className="w-4 h-4 text-white" />
            </button>
          </div>

          {/* Sliding Transcript Panel */}
          {transcriptionEnabled && (
            <>
              {/* Floating Transcript Button (when minimized) */}
              {!transcriptPanelOpen && (
                <button
                  onClick={() => setTranscriptPanelOpen(true)}
                  className="absolute top-2 right-2 bg-purple-600 hover:bg-purple-700 text-white px-3 py-1.5 rounded-lg shadow-lg transition-all duration-200 text-xs font-medium"
                  title="Show transcript"
                >
                  Transcript ({transcriptMessages.length})
                </button>
              )}

              {/* Transcript Panel - slides up from text area */}
              <div 
                className={`absolute bottom-full left-0 right-0 bg-gray-800/95 backdrop-blur-sm border-t border-gray-600 transition-all duration-300 ease-in-out overflow-hidden ${
                  transcriptPanelOpen ? 'max-h-32' : 'max-h-0'
                }`}
              >
                {/* Transcript Header */}
                <div className="flex items-center justify-between px-4 py-2 border-b border-gray-600">
                  <div className="flex items-center space-x-2">
                    <div className="w-2 h-2 bg-purple-400 rounded-full animate-pulse"></div>
                    <h3 className="text-white font-medium text-sm">Transcript</h3>
                    <span className="text-gray-400 text-xs">({transcriptMessages.length})</span>
                  </div>
                  <div className="flex items-center space-x-1">
                    <button
                      onClick={() => setTranscriptPanelOpen(!transcriptPanelOpen)}
                      className="text-gray-400 hover:text-white transition-colors p-1"
                      title={transcriptPanelOpen ? "Minimize transcript" : "Show transcript"}
                    >
                      <ChevronDown className="w-4 h-4" />
                    </button>
                    <button
                      onClick={() => setTranscriptMessages([])}
                      className="text-gray-400 hover:text-red-400 transition-colors p-1"
                      title="Clear transcript"
                    >
                      <X className="w-3 h-3" />
                    </button>
                  </div>
                </div>

                {/* Transcript Content */}
                <div className="overflow-y-auto p-3 space-y-2" style={{ height: '80px' }}>
                  {transcriptMessages.length === 0 ? (
                    <div className="flex items-center justify-center h-full text-gray-400">
                      <div className="text-center">
                        <div className="w-6 h-6 mx-auto mb-1 opacity-50">
                          <svg viewBox="0 0 24 24" fill="currentColor">
                            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
                          </svg>
                        </div>
                        <p className="text-xs">Transcript appears here</p>
                      </div>
                    </div>
                  ) : (
                    transcriptMessages.map((message, index) => (
                      <div key={message.id} className="flex space-x-2">
                        <div className="flex-shrink-0">
                          <div className={`w-6 h-6 rounded-full flex items-center justify-center text-xs font-medium ${
                            message.speaker === 'user' 
                              ? 'bg-blue-600 text-white' 
                              : 'bg-purple-600 text-white'
                          }`}>
                            {message.speaker === 'user' 
                              ? 'U' 
                              : (message.stakeholderName || message.speaker).split(' ').map(n => n[0]).join('').slice(0, 1)
                            }
                          </div>
                        </div>
                        <div className="flex-1 min-w-0">
                          <div className="flex items-center space-x-1 mb-1">
                            <span className="text-white font-medium text-xs">
                              {message.speaker === 'user' ? 'You' : (message.stakeholderName || message.speaker)}
                            </span>
                            <span className="text-gray-500 text-xs">
                              {new Date(message.timestamp).toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}
                            </span>
                          </div>
                          <p className="text-gray-200 text-xs leading-relaxed">{message.content}</p>
                        </div>
                      </div>
                    ))
                  )}
                  <div ref={transcriptEndRef} />
                </div>
              </div>
            </>
          )}
        </div>

      </div>
    </div>
  );
};// Updated audio button behavior
// Audio button reverted to start/stop
// Fix for persistent feedback message
// Persistent feedback fix applied
